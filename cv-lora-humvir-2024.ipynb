{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5w39f_fnY9"
   },
   "source": [
    "esm based dynamic model (not using static embeds).\n",
    "\n",
    "+ Use HF Trainer, LORA:\n",
    "  * https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "\n",
    "Use TF:\n",
    "*  🇰https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb#scrollTo=de8419b5\n",
    "* Torch based /Trainer example:  https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=49dcba23\n",
    "\n",
    "\n",
    "\n",
    "* NOte for pytorch training could use trainer maybe, and mixed precision - https://huggingface.co/docs/transformers/v4.18.0/en/performance#fp16-training\n",
    "\n",
    "\n",
    "* QLORA finetuning: https://huggingface.co/blog/AmelieSchreiber/esm2-ptm\n",
    "  * https://huggingface.co/blog/AmelieSchreiber/esmbind   (token level)\n",
    "\n",
    "* Another lora, qlora example - may use too much mem/bug : https://github.com/huggingface/peft/issues/1023\n",
    "* Default trainer (`AutoModelForSequenceClassification`) + Lora https://huggingface.co/docs/peft/task_guides/image_classification_lora\n",
    "   * seq cls with lora - maybe `task_type=\"SEQ_CLS\"` ? https://github.com/huggingface/peft/blob/main/docs/source/task_guides/ptuning-seq-classification.md\n",
    "* https://www.kaggle.com/code/andregrbnr/protein-sequence-classification - lora modules to save ??\n",
    "\n",
    "  * ESM2-Lora mem bug (also accel data loop) ? https://github.com/huggingface/peft/issues/1023\n",
    "\n",
    "\n",
    "* QLORA: https://huggingface.co/blog/AmelieSchreiber/esm2-ptm\n",
    "  * `36 batch size` with esm-150M !\n",
    "\n",
    "* lora peft - classifier layer weight saving issue?  https://github.com/huggingface/peft/issues/577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3049,
     "status": "ok",
     "timestamp": 1706644628798,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "kalvvWZZgM0E",
    "outputId": "f323c22c-784c-4935-ef81-4b61183e3a3d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GO4tbwSU1pa9"
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge google-colab -y\n",
    "\n",
    "# !pip install tensorflow  -U -q # ankh\n",
    "\n",
    "# !pip install torch  -U -q # fair-esm # seqeval\n",
    "# !pip install transformers peft accelerate datasets evaluate bitsandbytes -U -q # --user\n",
    "# !pip install peft bitsandbytes -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_cRmHN0j709"
   },
   "source": [
    "* Use the unirpot fasta file I downloaded and uploaded to my drive\n",
    "\n",
    "`/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_90.fasta`\n",
    "\n",
    "* Download fasta from: `https://www.uniprot.org/uniref/?query=uniprot:(keyword%3A%22Transmembrane+%5BKW-0812%5D%22+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22)+identity:0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kwgVw-CN1OgJ"
   },
   "outputs": [],
   "source": [
    "#### DATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/swp_human_viri_all_embed_esm.parquet\" ## ESM1B embedding (max len 1022)\n",
    "# DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-train.csv.gz\"## TRAIN\n",
    "# DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "# DATA_PATH = \"/kaggle/input/humvir-proteins/hum_vir_swp-globalEmbed-train.csv/hum_vir_swp-globalEmbed-train.csv\"\n",
    "DATA_PATH = \"hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "\n",
    "## TEST data:\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-test.csv.gz\"## TRAIN\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "TEST_DATA_PATH = \"hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "\n",
    "# ## metadata for all reviewed/swissprot human + virus proteins\n",
    "# METADATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/SWP_human_viruses_all.xlsx\"\n",
    "\n",
    "TARGET_COL = \"virus\" ## use for filtering data into 1 class\n",
    "\n",
    "MAX_LEN = 1024#768#530#480 # exclude sequences longer than this. (Not merely truncate)\n",
    "\n",
    "FAST_RUN = False#True\n",
    "SAVE_MODEL = False#True\n",
    "\n",
    "\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/trained_esm_lora_trainer_model\"\n",
    "MODEL_DRIVE_SAVE_PATH = \"small_trained_esm_lora_trainer_model\"\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/kaggle/input/humvir-proteins/qlora/qlora\" # saved + reuploadedon kaggle\n",
    "\n",
    "TRAIN_MODEL = True#False\n",
    "LOAD_TRAINED = False#True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa-7WUuSfnZC"
   },
   "source": [
    "# Embed sequences in a FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11612,
     "status": "ok",
     "timestamp": 1706644666124,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "hKw7JD3_fnZD",
    "outputId": "ffa04bad-8a62-447c-f1cd-466ebec4c602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 18:03:50.309233: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-17 18:03:50.328284: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-17 18:03:50.328306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-17 18:03:50.328865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-17 18:03:50.332559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-17 18:03:50.731152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/ipykernel_2520/2218226195.py:19: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "# from Bio import SeqIO\n",
    "import torch\n",
    "# import esm\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "# from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "## https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "# from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "# import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForSequenceClassification, AutoPeftModel\n",
    "\n",
    "## could use transformer pipeline for inference;\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "# from tqdm.auto import tqdm\n",
    "# pipe = pipeline(\"text-classification\", model=\"facebook/wav2vec2-base-960h\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "# # alt \n",
    "# # wandb.init(project='qlora_humvir')# ; or args = TrainingArguments(report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r8gFb91iUc8N"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "# # Use the accelerator\n",
    "# ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # שבבקךקרשאםר צשטנק בשודקד ןדדוקד?\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\") #\"bf16\") #bf16\") # fp16 # orig used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H1bosQnEwXvf"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2#4\n",
    "bch_size = 16#36#32#8#3#2\n",
    "\n",
    "# opt = Adafactor(3e-4)##AdamWeightDecay(1e-4) #default: AdamWeightDecay(2e-5)\n",
    "# opt = AdamWeightDecay(5e-4)#(1e-3)\n",
    "# opt = Adam(8e-4)\n",
    "\n",
    "if FAST_RUN:\n",
    "    num_epochs = 2\n",
    "    # bch_size = 16\n",
    "    bch_size = 32\n",
    "    MAX_LEN = int(MAX_LEN//1.24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dEGNJ6xlxQvC"
   },
   "outputs": [],
   "source": [
    "# model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n",
    "model_checkpoint =  \"facebook/esm2_t30_150M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "if FAST_RUN:\n",
    "#     model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "  model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4653,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "Eggiq94CubAw",
    "outputId": "9ab29551-cfb8-47d0-9b6d-ff6457e3ca09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25117\n",
      "Sequence        25117\n",
      "virus               2\n",
      "Length           1483\n",
      "Cluster name    20950\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>virus</th>\n",
       "      <th>Length</th>\n",
       "      <th>Cluster name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>Cluster: DET1- and DDB1-associated protein 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...</td>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>Cluster: Protein MGF 360-19R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>Cluster: Pyrin domain-containing protein 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>Cluster: Protein FAM9B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...</td>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>Cluster: TRAF2 and NCK-interacting protein kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>MKWKALFTAAILQAQLPITEAQSFGLLDPKLCYLLDGILFIYGVIL...</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>Cluster: T-cell surface glycoprotein CD3 zeta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>MKVNRETKRLYVGGLSQDISEADLQNQFSRFGEVSDVEIITRKDDQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1167</td>\n",
       "      <td>Cluster: Nucleolar protein 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>MAGVGDAAAPGEGGGGGVDGPQRDGRGEAEQPGGSGGQGPPPAPQL...</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>Cluster: Transmembrane anterior posterior tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>MGASVSRGRAARVPAPEPEPEEALDLSQLPPELLLVVLSHVPPRTL...</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "      <td>Cluster: F-box only protein 27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>MEVLIGDPITTCLSPSVYDIICNLGFQLRENCDINSIVTQNGEVCW...</td>\n",
       "      <td>0</td>\n",
       "      <td>678</td>\n",
       "      <td>Cluster: Endoplasmic reticulum membrane-associ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25117 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sequence  virus  Length  \\\n",
       "0     MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...      0     102   \n",
       "1     MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...      1     362   \n",
       "2     MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...      0      97   \n",
       "3     MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...      0     186   \n",
       "4     MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...      0    1360   \n",
       "...                                                 ...    ...     ...   \n",
       "4772  MKWKALFTAAILQAQLPITEAQSFGLLDPKLCYLLDGILFIYGVIL...      0     164   \n",
       "4773  MKVNRETKRLYVGGLSQDISEADLQNQFSRFGEVSDVEIITRKDDQ...      0    1167   \n",
       "4774  MAGVGDAAAPGEGGGGGVDGPQRDGRGEAEQPGGSGGQGPPPAPQL...      0     567   \n",
       "4775  MGASVSRGRAARVPAPEPEPEEALDLSQLPPELLLVVLSHVPPRTL...      0     283   \n",
       "4776  MEVLIGDPITTCLSPSVYDIICNLGFQLRENCDINSIVTQNGEVCW...      0     678   \n",
       "\n",
       "                                           Cluster name  \n",
       "0          Cluster: DET1- and DDB1-associated protein 1  \n",
       "1                          Cluster: Protein MGF 360-19R  \n",
       "2            Cluster: Pyrin domain-containing protein 2  \n",
       "3                                Cluster: Protein FAM9B  \n",
       "4     Cluster: TRAF2 and NCK-interacting protein kinase  \n",
       "...                                                 ...  \n",
       "4772  Cluster: T-cell surface glycoprotein CD3 zeta ...  \n",
       "4773                       Cluster: Nucleolar protein 8  \n",
       "4774  Cluster: Transmembrane anterior posterior tran...  \n",
       "4775                     Cluster: F-box only protein 27  \n",
       "4776  Cluster: Endoplasmic reticulum membrane-associ...  \n",
       "\n",
       "[25117 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_parquet(DATA_PATH) # numpy to pandas\n",
    "df = pd.read_csv(DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    "df_test = pd.read_csv(TEST_DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    " ## lengths of all the seqs\n",
    "    \n",
    "## concat all data - for CV\n",
    "df = pd.concat([df,df_test])\n",
    "del df_test\n",
    "print(df.shape[0])\n",
    "print(df.nunique())\n",
    "\n",
    "if FAST_RUN:\n",
    "#   # df.loc[df[\"Length\"]>100]\n",
    "    df = df.sample(frac=0.1)\n",
    "#     df_test = df_test.sample(frac=0.1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[df[\"Length\"]>500].sample(500)\n",
    "# print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.groupby([\"virus\"])[\"Cluster name\"].nunique().describe().round(2)\n",
    "except:()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "fkeYBgfafVoy",
    "outputId": "e8f72139-fcad-41d6-f9df-7fb73571df63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18418.0</td>\n",
       "      <td>471.71</td>\n",
       "      <td>307.70</td>\n",
       "      <td>11.0</td>\n",
       "      <td>242.25</td>\n",
       "      <td>398.0</td>\n",
       "      <td>621.0</td>\n",
       "      <td>1534.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6699.0</td>\n",
       "      <td>377.96</td>\n",
       "      <td>297.31</td>\n",
       "      <td>11.0</td>\n",
       "      <td>150.00</td>\n",
       "      <td>293.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1531.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count    mean     std   min     25%    50%    75%     max\n",
       "virus                                                             \n",
       "0      18418.0  471.71  307.70  11.0  242.25  398.0  621.0  1534.0\n",
       "1       6699.0  377.96  297.31  11.0  150.00  293.0  512.0  1531.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"virus\"])[\"Length\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "rm2Ms4BmQ4wn",
    "outputId": "a8edf73e-0541-43dd-8537-10b64de09661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean         0.27\n",
      "sum       6699.00\n",
      "count    25117.00\n",
      "Name: virus, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    25117.0\n",
       "mean       447.0\n",
       "std        308.0\n",
       "min         11.0\n",
       "25%        213.0\n",
       "50%        371.0\n",
       "75%        592.0\n",
       "max       1534.0\n",
       "Name: Length, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"virus\"].agg([\"mean\",\"sum\",\"count\"]).round(2))\n",
    "df[\"Length\"].describe().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "F8VNnHUwRQRm",
    "outputId": "c5502a31-ec5d-417d-fb9e-87e1de7d358d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    25117.0\n",
      "mean       446.7\n",
      "std        307.8\n",
      "min         11.0\n",
      "25%        213.0\n",
      "50%        371.0\n",
      "75%        592.0\n",
      "max       1534.0\n",
      "Name: Length, dtype: float64\n",
      "mean         0.27\n",
      "sum       6699.00\n",
      "count    25117.00\n",
      "Name: virus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# df = df.loc[df[\"Length\"]<=2*MAX_LEN].reset_index(drop=True)\n",
    "# df_test = df_test.loc[df_test[\"Length\"]<=2*MAX_LEN].reset_index(drop=True)\n",
    "\n",
    "print(df[\"Length\"].describe().round(1))\n",
    "print(df[\"virus\"].agg([\"mean\",\"sum\",\"count\"]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence        25117\n",
       "virus               2\n",
       "Length           1483\n",
       "Cluster name    20950\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PcMwJBkUov07"
   },
   "outputs": [],
   "source": [
    "# ## metadata about all sequences, can be used to identify and to define targets/labels\n",
    "# df_meta = pd.read_excel(METADATA_PATH).dropna(how=\"all\",axis=1)\n",
    "# df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSXy_OzjhIB7"
   },
   "source": [
    "### example pretrained fb/torch:\n",
    "* https://github.com/facebookresearch/esm#getting-started-with-this-repo-\n",
    "\n",
    "* Transformers + trainer example : (mlm case): https://github.com/facebookresearch/esm/discussions/556\n",
    "* keras models supported / via HF?\n",
    "  * https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "\n",
    "\n",
    "  TF finetuning example (sequence evel?):\n",
    "  https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb#scrollTo=4b26b828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CDdMQ98rii9t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForTokenClassification, TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tcssXXfaVYns"
   },
   "outputs": [],
   "source": [
    "ID2LABEL = {\n",
    "    0: \"Human\",\n",
    "    1: \"Virus\"\n",
    "}\n",
    "LABEL2ID = {v: k for k, v in ID2LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rR9-cKgPjzW2"
   },
   "outputs": [],
   "source": [
    "train_sequences = df[\"Sequence\"].tolist()\n",
    "train_labels = df[\"virus\"].tolist()\n",
    "train_groups = df[\"Cluster name\"].tolist()\n",
    "\n",
    "# # train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)#,stratify=labels)\n",
    "\n",
    "# test_sequences = df_test[\"Sequence\"].tolist()\n",
    "# test_labels = df_test[\"virus\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(base_model_name_or_path=model_checkpoint,\n",
    "#                         init_lora_weights=\"loftq\", loftq_config=loftq_config,\n",
    "    task_type= \"SEQ_CLS\",#TaskType.SEQ_CLS, ## disabling helps?? (then get \"ValueError: Attempting to unscale FP16 gradients.\")\n",
    "    inference_mode=False, r= 2 if FAST_RUN else 4, #16,\n",
    "    lora_alpha=8,\n",
    "#     lora_dropout=0.05,\n",
    "    use_rslora = True,\n",
    "    bias= \"all\"#\"lora_only\",#\"none\",#\"lora_only\",#\"none\",#\"all\",\n",
    "    # target_modules=[\n",
    "    #     \"query\", \"key\", \"value\",\n",
    "    #                 \"EsmSelfOutput.dense\",\n",
    "    #         \"EsmIntermediate.dense\",\n",
    "    #         \"EsmOutput.dense\",\n",
    "    #                 # \"word_embeddings\",\n",
    "    #                 # \"EsmClassificationHead.dense\", ## not sure if works/changes anything\n",
    "    #                 # \"out_proj\",\n",
    "    #                 # \"classifier\" # fails\n",
    "    #                 ],\n",
    "#         target_modules=  target_modules#\"all-linear\"#modules_list,\n",
    "           ,target_modules=  \"all-linear\"\n",
    "                         \n",
    "    # # ### https://www.kaggle.com/code/andregrbnr/protein-sequence-classification\n",
    "#      ,modules_to_save= \"all-linear\",\n",
    "#                          [#\"decode_head\",\n",
    "# #                       \"classifier\",\n",
    "#          \"EsmClassificationHead\",\n",
    "#          'classifier.dense', 'classifier.out_proj',\n",
    "#          \"pooler\",\n",
    "# # #                      'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "#                      ]\n",
    "    ## 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "    # modules_to_save=[\"classifier\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mEoRinaIznOa"
   },
   "outputs": [],
   "source": [
    "##https://huggingface.co/docs/peft/main/en/developer_guides/quantization\n",
    "## lotfQ config - for this, do not initialize as quantized!\n",
    "# from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "# loftq_config = LoftQConfig(loftq_bits=4)\n",
    "\n",
    "## https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True, # disable to train ok;\n",
    "#   load_in_8bit=True, # alt\n",
    "# load_in_4bit=False,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "  # llm_int8_has_fp16_weight= True, # try alt\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1706644674751,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "k_KUug1Lh5bH",
    "outputId": "14e7d50d-cdfe-4965-9f83-704a4a821e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load ESM-2 model\n",
    "## smallest: esm2_t6_8M_UR50D\n",
    "## 2d smallest\n",
    "## large: esm2_t33_650M_UR50D\n",
    "\n",
    " # ElnaggarLab/ankh-base\n",
    " ### https://github.com/agemagician/Ankh/blob/main/examples/binary_classification_solubility_task.ipynb - different model?\n",
    " #  https://github.com/agemagician/Ankh#models   - 450M model size # model_checkpoint =   \"ElnaggarLab/ankh-base\"\n",
    "\n",
    "model_max_len = min(1024,MAX_LEN) # 800\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "#                                           padding= False#True # orig\n",
    "                                          padding= True# alt\n",
    "                                          ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "num_labels = 2 # max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "print(\"Num labels:\", num_labels)\n",
    "##ORIG:\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\") # worked, orig\n",
    "\n",
    "## try this now, alt:\n",
    "# model = EsmForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "# model = TFEsmForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\")\n",
    "\n",
    "## https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=fc164b49 # uses trainer\n",
    "# ##\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, #num_labels=num_labels,\n",
    "# #                                                            problem_type=\"single_label_classification\", # was enabled\n",
    "# #                                                            load_in_4bit=True, # disable to train ok\n",
    "# #                                                            load_in_4bit= False,\n",
    "# #                                                             quantization_config=nf4_config,\n",
    "#                                                           #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "# #                                                            device_map= \"cuda:0\",#\"auto\",\n",
    "#                                                            device_map=\"auto\",\n",
    "#                                                           num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "# #                                                             trust_remote_code=True,\n",
    "#                                                           )\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # prepares the whole model for kbit training\n",
    "\n",
    "# last_layer_num = model.num_layers ## 33 for esm2_t33_650M_UR50D\n",
    "# print(last_layer_num )\n",
    "\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_model_lora(model_checkpoint):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                           problem_type=\"single_label_classification\", # was enabled\n",
    "#                                                            load_in_4bit=True, \n",
    "#                                                             quantization_config=nf4_config,\n",
    "                                                          #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "                                                           device_map=\"auto\",\n",
    "                                                          num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "                                                          )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # prepares the whole model for kbit training\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 717,135 || all params: 34,652,905 || trainable%: 2.069480177780189\n"
     ]
    }
   ],
   "source": [
    "model = get_raw_model_lora(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 717,135 || all params: 34,652,905 || trainable%: 2.069480177780189\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706644674751,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "i8amY00Q3HC3",
    "outputId": "9920c745-cdfc-445b-f9a5-39e45a147a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): EsmForSequenceClassification(\n",
      "      (esm): EsmModel(\n",
      "        (embeddings): EsmEmbeddings(\n",
      "          (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
      "        )\n",
      "        (encoder): EsmEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x EsmLayer(\n",
      "              (attention): EsmAttention(\n",
      "                (self): EsmSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (rotary_embeddings): RotaryEmbedding()\n",
      "                )\n",
      "                (output): EsmSelfOutput(\n",
      "                  (dense): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (intermediate): EsmIntermediate(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=480, out_features=1920, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Identity()\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=4, out_features=1920, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (output): EsmOutput(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1920, out_features=480, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Identity()\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1920, out_features=4, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (contact_head): EsmContactPredictionHead(\n",
      "          (regression): lora.Linear(\n",
      "            (base_layer): Linear(in_features=240, out_features=1, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=240, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (activation): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): EsmClassificationHead(\n",
      "          (dense): lora.Linear(\n",
      "            (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (out_proj): lora.Linear(\n",
      "            (base_layer): Linear(in_features=480, out_features=2, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=2, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "        )\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): EsmClassificationHead(\n",
      "            (dense): lora.Linear(\n",
      "              (base_layer): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=4, out_features=480, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (out_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=480, out_features=2, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=480, out_features=4, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=4, out_features=2, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # # model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False) # prepares the whole model for kbit training\n",
    "\n",
    "print(model)\n",
    "#### model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After we wrap our base model model with PeftModel along with the config, we get a new model where only the LoRA parameters are trainable (so-called “update matrices”) while the pre-trained parameters are kept frozen. These include the parameters of the randomly initialized classifier parameters too. This is NOT we want when fine-tuning the base model on our custom dataset. To ensure that the classifier parameters are also trained, we specify modules_to_save. This also ensures that these modules are serialized alongside the LoRA trainable parameters when using utilities like save_pretrained() and push_to_hub().\n",
    "\n",
    "In addition to specifying the target_modules within LoraConfig, we also need to specify the modules_to_save. When we wrap our base model with PeftModel and pass the configuration, we obtain a new model in which only the LoRA parameters are trainable, while the pre-trained parameters and the randomly initialized classifier parameters are kept frozen. However, we do want to train the classifier parameters. By specifying the modules_to_save argument, we ensure that the classifier parameters are also trainable, and they will be serialized alongside the LoRA trainable parameters when we use utility functions like save_pretrained() and push_to_hub().\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1706644676196,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "us1e8ZABC-Ux",
    "outputId": "eca9e5b0-9d94-46a2-eee7-f140620f5cc7"
   },
   "outputs": [],
   "source": [
    "## https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "# https://huggingface.co/docs/peft/task_guides/token-classification-lora\n",
    "### target modules?? https://discuss.huggingface.co/t/esm-2-qlora-gradient-checkpointing-not-compatible/53505/2\n",
    "## could also set to all linear? \n",
    "\n",
    "### https://huggingface.co/docs/peft/task_guides/semantic_segmentation_lora#wrap-the-base-model-as-a-peftmodel-for-lora-training\n",
    "\n",
    "\n",
    "# get_peft_model(model, peft_config)\n",
    "# # model = get_peft_model(model, peft_config,mixed=True) # mixed prevents peft save, but othewise, 8bit error?\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Aw4wKUDNFnBj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_tokenized = tokenizer(train_sequences,  truncation=True,max_length=model_max_len,padding=False) # padding=True,\\ntest_tokenized = tokenizer(test_sequences,  truncation=True,max_length=model_max_len,padding=False) # padding=True,\\n\\ntrain_dataset = Dataset.from_dict(train_tokenized)\\ntest_dataset = Dataset.from_dict(test_tokenized)\\n\\ntrain_dataset = train_dataset.add_column(\"labels\", train_labels)\\ntest_dataset = test_dataset.add_column(\"labels\", test_labels)\\ntrain_dataset\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Warning - longer than allowed length - 1024\n",
    "\"\"\"\n",
    "train_tokenized = tokenizer(train_sequences,  truncation=True,max_length=model_max_len,padding=False) # padding=True,\n",
    "test_tokenized = tokenizer(test_sequences,  truncation=True,max_length=model_max_len,padding=False) # padding=True,\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "y_4o9iEZFLBZ"
   },
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# # # Use the accelerator\n",
    "# # ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # # # accelerator = Accelerator(mixed_precision=\"fp16\") # fp16 # orig used\n",
    "# model = accelerator.prepare(model)\n",
    "\n",
    "# train_dataset = accelerator.prepare(train_dataset)\n",
    "# test_dataset = accelerator.prepare(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEcix1ZWfQg-"
   },
   "source": [
    "With 35M model :\n",
    "```\n",
    "Default (1024) max length\n",
    "batch_size=8\n",
    "\n",
    "opt = Adafactor(1e-4)##AdamWeightDecay(1e-4) #default: AdamWeightDecay(2e-5)\n",
    "model.compile(optimizer=opt, metrics=[\"accuracy\"],\n",
    "              loss=\"BinaryCrossentropy\")\n",
    "3813/3813 [==============================] - 2625s 661ms/step - loss: 0.2452 - accuracy: 0.9104 - val_loss: 0.1622 - val_accuracy: 0.9363\n",
    "Epoch 2/3\n",
    "3813/3813 [==============================] - 2518s 661ms/step - loss: 0.1218 - accuracy: 0.9597 - val_loss: 0.1533 - val_accuracy: 0.9463\n",
    "Epoch 3/3\n",
    "3813/3813 [==============================] - 2523s 662ms/step - loss: 0.0730 - accuracy: 0.9799 - val_loss: 0.1978 - val_accuracy: 0.9436\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Q_3tSKYxJJtE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,roc_auc_score\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_metric\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "    \n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"roc_auc\":roc_auc_score(labels,pred.predictions[:,1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706644733230,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "DUSLawmyM_GS",
    "outputId": "86a59590-af50-4828-e0de-a9da50297f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t12_35M_UR50D\n"
     ]
    }
   ],
   "source": [
    "# ### AttributeError: 'TFEsmForSequenceClassification' object has no attribute 'to'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "print(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ft-humVir\",\n",
    "#     f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "    per_device_train_batch_size=bch_size,\n",
    "    # per_device_eval_batch_size=int(2*bch_size),\n",
    "    # gradient_accumulation_steps= 2, #4,\n",
    "    gradient_checkpointing=True, # maybe causes slowdown? \n",
    "    fp16=True,\n",
    "    # bf16=True, # needs ampere, not supported\n",
    "    # tf32=True,\n",
    "#         torch_compile = True,\n",
    "    optim = \"adamw_8bit\",#\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "    label_names = [\"labels\"],\n",
    "    learning_rate = 3e-4,#5e-4, #5e-3,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm = 0.9,\n",
    "#     weight_decay=0.001,\n",
    "    # eval_accumulation_steps = 2#8\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # warmup_ratio=0.02,\n",
    "    save_strategy= \"epoch\",#\"no\",\n",
    "    # output_dir=\".\",\n",
    "     no_cuda=False,\n",
    "     greater_is_better=True,\n",
    "     save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "    auto_find_batch_size = True, # new , reduces if oom\n",
    "    num_train_epochs=num_epochs,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model= \"roc_auc\",#\"accuracy\",\n",
    "    group_by_length=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hbj58ofX8umQ",
    "outputId": "a60712d1-b63b-4c6f-a988-d8e8b50a0c77"
   },
   "outputs": [],
   "source": [
    "# if TRAIN_MODEL:\n",
    "#     result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run CV\n",
    "\n",
    "150M , full data, withgradient checkpoint enabled: 2.2 min per epoch (4.4 total), for 500 (long) samples.\n",
    "```\n",
    " [42/42 04:41, Epoch 2/2]\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\tF1\tPrecision\tRecall\tRoc Auc\n",
    "1\tNo log\t0.344198\t0.868263\t0.868263\t0.868263\t0.868263\t0.938507\n",
    "2\tNo log\t0.255124\t0.880240\t0.880240\t0.880240\t0.880240\t0.958227\n",
    "```\n",
    "similar time when not using gradient checkpoint and double (32) batch szie - 4 min for. epochs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 717,135 || all params: 34,652,905 || trainable%: 2.069480177780189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='524' max='524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [524/524 39:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.186317</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.972603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.207600</td>\n",
       "      <td>0.188354</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.975146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 717,135 || all params: 34,652,905 || trainable%: 2.069480177780189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='524' max='524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [524/524 39:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.256977</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.950527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199500</td>\n",
       "      <td>0.261376</td>\n",
       "      <td>0.908863</td>\n",
       "      <td>0.908863</td>\n",
       "      <td>0.908863</td>\n",
       "      <td>0.908863</td>\n",
       "      <td>0.955372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 717,135 || all params: 34,652,905 || trainable%: 2.069480177780189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 54/524 03:32 < 31:56, 0.25 it/s, Epoch 0.20/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "# 'groups' contains group labels for each data point to ensure stratification by groups.\n",
    "\n",
    "n_splits = 3  # Number of folds\n",
    "if FAST_RUN:\n",
    "    n_splits = 2\n",
    "\n",
    "skf = StratifiedGroupKFold(n_splits=n_splits) \n",
    "\n",
    "all_sequences = df[\"Sequence\"].tolist()\n",
    "all_labels = df[\"virus\"].tolist()\n",
    "# groups = df[\"group\"].tolist()#df[\"Cluster name\"].tolist()\n",
    "groups = df[\"Cluster name\"].tolist()\n",
    "\n",
    "all_sequences = np.array(all_sequences)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Placeholder for predictions (2 vectors - ineffecient but simpler for now)\n",
    "all_predictions_labels = np.zeros(len(all_labels))\n",
    "all_predictions_proba = np.zeros(len(all_labels))\n",
    "i=0\n",
    "# Loop through each fold\n",
    "for train_index, test_index in skf.split(all_sequences, all_labels, groups):\n",
    "    print(\"fold\",i)\n",
    "    i =+ 1\n",
    "    model = get_raw_model_lora(model_checkpoint)\n",
    "#     train_sequences, test_sequences = all_sequences[train_index], all_sequences[test_index]\n",
    "    train_labels, test_labels = all_labels[train_index], all_labels[test_index]\n",
    "    train_sequences = [all_sequences[i] for i in train_index]\n",
    "    test_sequences = [all_sequences[i] for i in test_index]\n",
    "\n",
    "    train_tokenized = tokenizer(train_sequences,  truncation=True,max_length=model_max_len, padding=True)\n",
    "    test_tokenized = tokenizer(test_sequences,  truncation=True,max_length=model_max_len, padding=True)\n",
    "\n",
    "    train_dataset = Dataset.from_dict(train_tokenized)\n",
    "    test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "    train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "    test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args,\n",
    "                  train_dataset=train_dataset,eval_dataset=test_dataset,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "    \n",
    "# #     # Train on the fold\n",
    "    trainer.train()\n",
    "    # Predict on the test fold\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    # The output of the predict() method is another named tuple with three fields: predictions, label_ids, and metrics.\n",
    "    # https://huggingface.co/learn/nlp-course/en/chapter3/3\n",
    "#     all_predictions[test_index] = predictions.squeeze() # orig from tf model\n",
    "    all_predictions_labels[test_index] =predictions.label_ids\n",
    "    ## proba (double check axis)!:  \n",
    "    all_predictions_proba[test_index] =   torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:,1]\n",
    "    \n",
    "print(\"Cross-validation complete. Predictions are stored in 'all_predictions'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds = df.copy()\n",
    "df_all_preds[\"pred_score_1\"] =all_predictions_proba.round(5)\n",
    "df_all_preds[\"pred_1\"] = all_predictions_labels.astype(int)\n",
    "\n",
    "test_mistakes_mask = df_all_preds[\"virus\"]!=all_predictions.astype(int)\n",
    "print(test_mistakes_mask.sum(),\"#mistakes\")\n",
    "print(100*(test_mistakes_mask.mean()).round(3),\"%mistakes\")\n",
    "df_all_preds[\"mistake\"] = test_mistakes_mask\n",
    "\n",
    "df_all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds.to_csv(\"cvLora_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuInD2PVjT_j"
   },
   "source": [
    "* https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "\n",
    "\n",
    "* Eval related batch size stuff: https://discuss.huggingface.co/t/batch-size-for-trainer-predict/3374/2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1f6tWCmDttwj5GsdTavt7m-dIH1-2f1P5",
     "timestamp": 1706088386277
    },
    {
     "file_id": "18pa4xD0P5vwL-MX30nrkeye78W7403TZ",
     "timestamp": 1702551198722
    },
    {
     "file_id": "1uwM47KOjpzSn2TylouVCrTiGuskjQtEd",
     "timestamp": 1690279074762
    },
    {
     "file_id": "1_4meskHUbh7-dkjF-ajOT_wazp1vxzC6",
     "timestamp": 1630488037990
    },
    {
     "file_id": "1325niNI11d1AGvkudzFonUaYu-_IpRSv",
     "timestamp": 1629371463407
    },
    {
     "file_id": "1eeHOXM2csXDuY2ysmo-npc7dWtr9hcKJ",
     "timestamp": 1629281016353
    },
    {
     "file_id": "https://github.com/sacdallago/bio_embeddings/blob/develop/notebooks/embed_fasta_sequences.ipynb",
     "timestamp": 1629109653928
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4380199,
     "sourceId": 7529659,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
