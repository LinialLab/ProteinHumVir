{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5w39f_fnY9"
   },
   "source": [
    "* Load model trained and saved in previous stage\n",
    "* * Lora/QLORa model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kalvvWZZgM0E"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# # drive.mount('/content/drive/MyDrive/proteins')\n",
    "# # drive.mount('/content/drive/MyDrive/proteins/New Protein-Virus anom project')\n",
    "# # !cd /MyDrive/proteins/New\\ Protein-Virus\\ anom\\ project\n",
    "\n",
    "# # %cd /MyDrive/proteins/New\\ Protein-Virus\\ anom\\ project\n",
    "# # %cd drive/MyDrive/proteins\n",
    "# # %cd /New\\ Protein-Virus\\ anom\\ project\n",
    "\n",
    "\n",
    "# %cd /content/drive/MyDrive/proteins/New\\ Protein-Virus\\ anom\\ project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GO4tbwSU1pa9"
   },
   "outputs": [],
   "source": [
    "# # !conda install -c conda-forge google-colab -y\n",
    "\n",
    "# # !pip install tensorflow  -U -q # ankh\n",
    "\n",
    "# # !pip install torch  -U -q # fair-esm # seqeval\n",
    "# !pip install transformers peft accelerate datasets evaluate bitsandbytes wandb -U -q # --user\n",
    "# # !pip install transformers peft bitsandbytes -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_cRmHN0j709"
   },
   "source": [
    "* Use the unirpot fasta file I downloaded and uploaded to my drive\n",
    "\n",
    "`/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_90.fasta`\n",
    "\n",
    "* Download fasta from: `https://www.uniprot.org/uniref/?query=uniprot:(keyword%3A%22Transmembrane+%5BKW-0812%5D%22+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22)+identity:0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kwgVw-CN1OgJ"
   },
   "outputs": [],
   "source": [
    "#### DATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/swp_human_viri_all_embed_esm.parquet\" ## ESM1B embedding (max len 1022)\n",
    "# DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-train.csv.gz\"## TRAIN\n",
    "# DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "# DATA_PATH = \"/kaggle/input/humvir-proteins/hum_vir_swp-globalEmbed-train.csv/hum_vir_swp-globalEmbed-train.csv\"\n",
    "DATA_PATH = \"hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "\n",
    "## TEST data:\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-test.csv.gz\"## TRAIN\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "# TEST_DATA_PATH = \"/kaggle/input/humvir-proteins/hum_vir_swp-globalEmbed-test.csv/hum_vir_swp-globalEmbed-test.csv\"\n",
    "TEST_DATA_PATH = \"hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "\n",
    "# ## metadata for all reviewed/swissprot human + virus proteins\n",
    "# METADATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/SWP_human_viruses_all.xlsx\"\n",
    "\n",
    "TARGET_COL = \"virus\" ## use for filtering data into 1 class\n",
    "\n",
    "MAX_LEN = 612#1024#768#530 # exclude sequences longer than this. (Not merely truncate)\n",
    "\n",
    "FAST_RUN = False#True\n",
    "SAVE_MODEL = False#True\n",
    "\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/trained_esm_lora_trainer_model\"\n",
    "# MODEL_DRIVE_SAVE_PATH = \"final\"\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/kaggle/input/humvir-proteins/qlora/qlora\" # saved + reuploadedon kaggle\n",
    "MODEL_DRIVE_SAVE_PATH =  \"./esm_lora_trainer_model_sml\" ## LAST used\n",
    "# MODEL_DRIVE_SAVE_PATH = \"./esm150_lora_trainer_model\" # nonexist\n",
    "TRAIN_MODEL = False\n",
    "LOAD_TRAINED =  True#False# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hKw7JD3_fnZD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 23:06:08.884410: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 23:06:08.902426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-11 23:06:08.902448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-11 23:06:08.903032: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 23:06:08.906540: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 23:06:09.314664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/ipykernel_9310/1981784629.py:15: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "# from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "## https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "# from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "# import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForSequenceClassification, AutoPeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "## could use transformer pipeline for inference;\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModelForSequenceClassification, get_peft_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForTokenClassification, TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,roc_auc_score\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_metric\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# pipe = pipeline(\"text-classification\", model=\"facebook/wav2vec2-base-960h\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7aF2oAL2_Fph"
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init(mode=\"disabled\")\n",
    "# # alt\n",
    "# wandb.init(project='qlora_humvir')# ; or args = TrainingArguments(report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r8gFb91iUc8N"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "# # Use the accelerator\n",
    "# ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # שבבקךקרשאםר צשטנק בשודקד ןדדוקד?\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\") #bf16\") # fp16 # orig used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dEGNJ6xlxQvC"
   },
   "outputs": [],
   "source": [
    "# model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t30_150M_UR50D\"\n",
    "model_checkpoint =  \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "if FAST_RUN:\n",
    "    # model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "  model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Eggiq94CubAw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>virus</th>\n",
       "      <th>Length</th>\n",
       "      <th>Cluster name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>Cluster: DET1- and DDB1-associated protein 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...</td>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>Cluster: Protein MGF 360-19R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>Cluster: Pyrin domain-containing protein 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>Cluster: Protein FAM9B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...</td>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>Cluster: TRAF2 and NCK-interacting protein kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20335</th>\n",
       "      <td>MDPDKQDALNSIENSIYRTAFKLQSVQTLCQLDLIDSSLIQQVLLR...</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>Cluster: Dystrotelin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20336</th>\n",
       "      <td>MLCPWRTANLGLLLILTIFLVAEAEGAAQPNNSLMLQTSKENHALA...</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>Cluster: Cell surface glycoprotein CD200 recep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>MCLRFFSPVPGSTSSATNVTMVVSAGPWSSEKAEMNILEINEKLRP...</td>\n",
       "      <td>0</td>\n",
       "      <td>421</td>\n",
       "      <td>Cluster: Putative neuroblastoma breakpoint fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20338</th>\n",
       "      <td>MASHAGQQHAPAFGQAARASGPTDGRAASRPSHRQGASEARGDPEL...</td>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>Cluster: Thymidine kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20339</th>\n",
       "      <td>MSASSLLEQRPKGQGNKVQNGSVHQKDGLNDDDFEPYLSPQARPNN...</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>Cluster: YTH domain-containing family protein 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20340 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sequence  virus  Length  \\\n",
       "0      MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...      0     102   \n",
       "1      MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...      1     362   \n",
       "2      MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...      0      97   \n",
       "3      MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...      0     186   \n",
       "4      MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...      0    1360   \n",
       "...                                                  ...    ...     ...   \n",
       "20335  MDPDKQDALNSIENSIYRTAFKLQSVQTLCQLDLIDSSLIQQVLLR...      0     578   \n",
       "20336  MLCPWRTANLGLLLILTIFLVAEAEGAAQPNNSLMLQTSKENHALA...      0     348   \n",
       "20337  MCLRFFSPVPGSTSSATNVTMVVSAGPWSSEKAEMNILEINEKLRP...      0     421   \n",
       "20338  MASHAGQQHAPAFGQAARASGPTDGRAASRPSHRQGASEARGDPEL...      1     376   \n",
       "20339  MSASSLLEQRPKGQGNKVQNGSVHQKDGLNDDDFEPYLSPQARPNN...      0     579   \n",
       "\n",
       "                                            Cluster name  \n",
       "0           Cluster: DET1- and DDB1-associated protein 1  \n",
       "1                           Cluster: Protein MGF 360-19R  \n",
       "2             Cluster: Pyrin domain-containing protein 2  \n",
       "3                                 Cluster: Protein FAM9B  \n",
       "4      Cluster: TRAF2 and NCK-interacting protein kinase  \n",
       "...                                                  ...  \n",
       "20335                               Cluster: Dystrotelin  \n",
       "20336  Cluster: Cell surface glycoprotein CD200 recep...  \n",
       "20337  Cluster: Putative neuroblastoma breakpoint fam...  \n",
       "20338                          Cluster: Thymidine kinase  \n",
       "20339    Cluster: YTH domain-containing family protein 2  \n",
       "\n",
       "[20340 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_parquet(DATA_PATH) # numpy to pandas\n",
    "df = pd.read_csv(DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    "df_test = pd.read_csv(TEST_DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\t\"Cluster name\"])\n",
    " ## lengths of all the seqs\n",
    "\n",
    "if FAST_RUN:\n",
    "#   # df.loc[df[\"Length\"]>100]\n",
    "    df = df.sample(frac=0.12,random_state=4)\n",
    "    df_test = df_test.sample(frac=0.35,random_state=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tcssXXfaVYns"
   },
   "outputs": [],
   "source": [
    "ID2LABEL = {\n",
    "    0: \"Human\",\n",
    "    1: \"Virus\"\n",
    "}\n",
    "LABEL2ID = {v: k for k, v in ID2LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rR9-cKgPjzW2"
   },
   "outputs": [],
   "source": [
    "# train_sequences = df[\"Sequence\"].tolist()\n",
    "# train_labels = df[\"virus\"].tolist()\n",
    "# train_groups = df[\"Cluster name\"].tolist()\n",
    "\n",
    "# train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)#,stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k_KUug1Lh5bH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 2\n"
     ]
    }
   ],
   "source": [
    "##https://huggingface.co/docs/peft/main/en/developer_guides/quantization\n",
    "## lotfQ config - for this, do not initialize as quantized!\n",
    "# from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "# loftq_config = LoftQConfig(loftq_bits=4)\n",
    "\n",
    "## https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "compute_dtype = getattr(torch, \"bfloat16\")\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True, # disable to train ok;\n",
    "  # load_in_8bit=True, # alt\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_has_fp16_weight =True,\n",
    "    llm_int8_threshold = 5.1,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    "    ,llm_int8_skip_modules=['classifier',\"EsmClassificationHead\"] # was enabled?\n",
    ")\n",
    "\n",
    "model_max_len = min(1024,MAX_LEN) # 800\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                          padding= True\n",
    "#                                           padding= True# alt\n",
    "                                          ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "num_labels = 2#max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "print(\"Num labels:\", num_labels)\n",
    "\n",
    "# ## https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=fc164b49 # uses trainer\n",
    "# ##\n",
    "\n",
    "# # model = AutoModelForSequenceClassification. # orig\n",
    "# # model = EsmForSequenceClassification.\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "#                                                            problem_type=\"single_label_classification\", # was enabled\n",
    "#                                                            # load_in_4bit=True, # disable to train ok\n",
    "#                                                             quantization_config=nf4_config,\n",
    "#                                                           #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "#                                                            # device_map= \"cuda:0\",#\"auto\",\n",
    "#                                                            device_map=\"auto\",\n",
    "#                                                           num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "#                                                             # trust_remote_code=True\n",
    "#                                                            # , from_tf=True,\n",
    "#                                                            # force_download =True,\n",
    "#                                                           )\n",
    "\n",
    "# # last_layer_num = model.num_layers ## 33 for esm2_t33_650M_UR50D\n",
    "# # print(last_layer_num )\n",
    "# model.train()\n",
    "# # model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYgBQoyt_Fpk"
   },
   "source": [
    "* `task_type= \"SEQ_CLS\"` - breaks - RuntimeError: only Tensors of floating point dtype can require gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "us1e8ZABC-Ux"
   },
   "outputs": [],
   "source": [
    "## https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "# https://huggingface.co/docs/peft/task_guides/token-classification-lora\n",
    "### target modules?? https://discuss.huggingface.co/t/esm-2-qlora-gradient-checkpointing-not-compatible/53505/2\n",
    "## could also set to all linear?\n",
    "\n",
    "### https://huggingface.co/docs/peft/task_guides/semantic_segmentation_lora#wrap-the-base-model-as-a-peftmodel-for-lora-training\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# peft_config = LoraConfig(base_model_name_or_path= MODEL_DRIVE_SAVE_PATH #model_checkpoint,\n",
    "#     ,task_type= TaskType.SEQ_CLS,#\"SEQ_CLS\",#, ## disabling helps?? (then get \"ValueError: Attempting to unscale FP16 gradients.\")\n",
    "#     inference_mode=False, r= 2 if FAST_RUN else 16, #16,\n",
    "#     lora_alpha=8,\n",
    "#     # lora_dropout=0.1,\n",
    "#     use_rslora = True,\n",
    "#     # bias= \"none\",#\"lora_only\",#\"none\",#\"all\",\n",
    "#     # bias=\"all\", #\"lora_only\",\n",
    "#     bias= \"all\",#\"lora_only\",\n",
    "#     target_modules=  \"all-linear\"\n",
    "\n",
    "# )\n",
    "\n",
    "# get_peft_model(model, peft_config)\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "peft_config = LoraConfig(base_model_name_or_path=model_checkpoint, ## MODEL_DRIVE_SAVE_PATH\n",
    "    task_type= TaskType.SEQ_CLS,\n",
    "                         r= 16,\n",
    "                         # lora_alpha=16, use_rslora = True,\n",
    "                         bias=\"all\",  #\"lora_only\",#\"all\", #\"lora_only\",\n",
    "           target_modules=  \"all-linear\" ## causes save score bug; but won't work without this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Aw4wKUDNFnBj"
   },
   "outputs": [],
   "source": [
    "## Warning - longer than allowed length - 1024\n",
    "def get_train_test_dataseqs(train_sequences,test_sequences,train_labels,test_labels,tokenizer):\n",
    "    train_tokenized = tokenizer( train_sequences )#,  truncation=True,max_length=model_max_len,padding=True,)\n",
    "    test_tokenized = tokenizer(test_sequences )#,  truncation=True,max_length=model_max_len,padding=True,)\n",
    "    train_dataset = Dataset.from_dict(train_tokenized)\n",
    "    test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "    train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "    test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q_3tSKYxJJtE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,roc_auc_score\n",
    "import evaluate\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        # 'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"roc_auc\":roc_auc_score(labels,pred.predictions[:,1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DUSLawmyM_GS"
   },
   "outputs": [],
   "source": [
    "# # ### AttributeError: 'TFEsmForSequenceClassification' object has no attribute 'to'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# print(model_name)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     # f\"{model_name}-finetuned-ft-humVir\",\n",
    "\n",
    "#      f\"{model_name}-zero_shot_ft\",\n",
    "#     # f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "#     per_device_train_batch_size=32,\n",
    "#     # per_device_eval_batch_size=int(1.5*bch_size),\n",
    "# #     gradient_accumulation_steps= 2, #4,\n",
    "#     gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "#     # bf16=True, # needs ampere, not supported ?\n",
    "#     # tf32=True,\n",
    "#         # torch_compile = True,\n",
    "#     optim = \"adamw_8bit\", #\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "#     # optim= \"adamw_bnb_8bit\", #\"paged_adamw_8bit\",\n",
    "#     label_names = [\"labels\"],\n",
    "#     learning_rate = 3e-4#2e-4 #5e-3,\n",
    "#     # lr_scheduler_type=\"cosine\",\n",
    "#     ,max_grad_norm = 0.95,\n",
    "#     # weight_decay=0.005,\n",
    "#     # eval_accumulation_steps = 2#8\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy= \"no\", #\"epoch\",#\"no\",\n",
    "#     #    load_best_model_at_end=True,\n",
    "#     # output_dir=\".\",\n",
    "#      no_cuda=False,\n",
    "#      greater_is_better=True,\n",
    "#      # save_total_limit=1,\n",
    "#   remove_unused_columns=False,\n",
    "#     auto_find_batch_size = True, # new , reduces if oom\n",
    "#     # num_train_epochs=num_epochs,\n",
    "\n",
    "#     metric_for_best_model= \"roc_auc\",#\"accuracy\",\n",
    "#     group_by_length=True,\n",
    "# )\n",
    "\n",
    "# # trainer = Trainer(model=model, args=training_args,\n",
    "# #                   train_dataset=train_dataset,eval_dataset=test_dataset,tokenizer=tokenizer,\n",
    "# #                   compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hbj58ofX8umQ"
   },
   "outputs": [],
   "source": [
    "# if TRAIN_MODEL:\n",
    "#     result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyaiUSi0DP6r"
   },
   "source": [
    "### load model for **comparison**\n",
    "* , untrained model ?\n",
    "*  https://huggingface.co/blog/AmelieSchreiber/esmbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9O24DVUe_Fpl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook/esm2_t33_650M_UR50D'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6jyS1XD9DPI0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "# # ESM2 base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\")\n",
    "# base_model.eval()\n",
    "\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model = prepare_model_for_kbit_training(base_model, use_gradient_checkpointing=True)\n",
    "base_model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2PwK94icaX_o"
   },
   "outputs": [],
   "source": [
    "# if FAST_RUN:\n",
    "#   print(\"Untrained base model\")\n",
    "#   print(Trainer(model=base_model, args=training_args,\n",
    "#                   train_dataset=train_dataset,eval_dataset=test_dataset,tokenizer=tokenizer,\n",
    "#                   compute_metrics=compute_metrics,).evaluate()) # worse than trained model - OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "u1pYpEezBD2E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./esm_lora_trainer_model_sml'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DRIVE_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OhDQrx_o_Fpm"
   },
   "outputs": [],
   "source": [
    "# trained_model = AutoModelForSequenceClassification.from_pretrained(MODEL_DRIVE_SAVE_PATH,\n",
    "#                                                               num_labels=num_labels,problem_type=\"single_label_classification\",\n",
    "#                                                                   id2label=ID2LABEL, label2id=LABEL2ID,) #ORIG, worked\n",
    "\n",
    "# # trained_model = PeftModel.from_pretrained(trained_model, MODEL_DRIVE_SAVE_PATH) #ORIG, worked\n",
    "# # alt\":\n",
    "# trained_model = PeftModelForSequenceClassification.from_pretrained(trained_model, MODEL_DRIVE_SAVE_PATH,\n",
    "#                                                                    config=peft_config) # alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R62G7yFduRiL"
   },
   "outputs": [],
   "source": [
    "###\n",
    "### it says it is not loading saved classifier weights?? but trainer pred gives ok results (better than with random model)?\n",
    "### load merge_unload model, not just poeft\n",
    "if LOAD_TRAINED:\n",
    "    \"\"\"\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\")\n",
    "    \"\"\"\n",
    "#   ### load pretrained, trained model?\n",
    "\n",
    "    # trained_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=num_labels,problem_type=\"single_label_classification\")\n",
    "    # print(\"loading PEFT config\")\n",
    "\n",
    "    # trained_model = PeftModelForSequenceClassification.from_pretrained(trained_model, MODEL_DRIVE_SAVE_PATH,\n",
    "    #                                                                    config=peft_config) # alt\n",
    "\n",
    "    trained_model = AutoModelForSequenceClassification.from_pretrained(MODEL_DRIVE_SAVE_PATH,\n",
    "                                                                  num_labels=num_labels,problem_type=\"single_label_classification\",\n",
    "                                                                      id2label=ID2LABEL, label2id=LABEL2ID,) #ORIG, worked\n",
    "    merged_model = trained_model ## assumes loaded non peft model\n",
    "\n",
    "    # # trained_model = PeftModel.from_pretrained(trained_model, MODEL_DRIVE_SAVE_PATH) #ORIG, worked\n",
    "    # # alt\":\n",
    "    # trained_model = PeftModelForSequenceClassification.from_pretrained(trained_model, MODEL_DRIVE_SAVE_PATH,\n",
    "    #                                                                    config=peft_config) # alt\n",
    "    ##\n",
    "#     trained_model = prepare_model_for_kbit_training(trained_model, use_gradient_checkpointing=True)\n",
    "\n",
    "else:\n",
    "    trained_model = trainer.model\n",
    "# trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_model = merged_model.merge_and_unload()\n",
    "merged_model.gradient_checkpointing_enable()\n",
    "merged_model = prepare_model_for_kbit_training(merged_model, use_gradient_checkpointing=True)\n",
    "merged_model = get_peft_model(merged_model, peft_config) # added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0Ja1T78_Fpm"
   },
   "source": [
    "# res are same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0PuAoFp4_Fpm"
   },
   "outputs": [],
   "source": [
    "# merged_model = trained_model.merge_and_unload()\n",
    "# merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6ibDslHLW04H"
   },
   "outputs": [],
   "source": [
    "## check if saved loaded ok?\n",
    "if FAST_RUN:\n",
    "  print(Trainer(model=trained_model, args=training_args,tokenizer=tokenizer,\n",
    "                    train_dataset=train_dataset,eval_dataset=test_dataset,\n",
    "                    compute_metrics=compute_metrics,).evaluate()) ## only ok if better than random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "97O_ItcTmgh3"
   },
   "outputs": [],
   "source": [
    "if FAST_RUN:\n",
    "  print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0P6iI7SH_Fpn"
   },
   "outputs": [],
   "source": [
    "# if FAST_RUN:\n",
    "#   print(\"merge and unload peft\")\n",
    "\n",
    "# print(Trainer(model=merged_model, args=training_args,tokenizer=tokenizer,\n",
    "#                     train_dataset=train_dataset,\n",
    "#               # eval_dataset=test_dataset,\n",
    "#                eval_dataset=train_dataset.select(range(700))\n",
    "#                     ,compute_metrics=compute_metrics,).evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j7v82ltx_Fpn"
   },
   "outputs": [],
   "source": [
    "# print(Trainer(model=base_model, args=training_args,tokenizer=tokenizer,\n",
    "#                     train_dataset=train_dataset,\n",
    "#               # eval_dataset=test_dataset,\n",
    "#                eval_dataset=train_dataset.select(range(700))\n",
    "#                     ,compute_metrics=compute_metrics,).evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EAsG--y_Fpn"
   },
   "source": [
    "* Use HF pipeline\n",
    "* Big bug with pipes - uses cpu? https://discuss.huggingface.co/t/text-classification-pipeline-very-slow-after-adding-padding-and-truncation-for-tokenizer/32173/3\n",
    "* https://stackoverflow.com/questions/77249578/how-to-create-a-dataset-with-huggingface-from-a-list-of-strings-to-fine-tune-lla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nA_Y_iGu_Fpn"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/main_classes/pipelines#pipeline-batching\n",
    "\n",
    "# https://stackoverflow.com/questions/68197664/how-to-take-just-the-score-from-huggingface-pipeline-sentiment-analysis\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                          # padding= False#True # orig\n",
    "                                          padding= True ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "# pipe = pipeline(\"text-classification\",model=merged_model, tokenizer=tokenizer, truncation=True, top_k=1,device_map=\"auto\") # top_k=None to get preds for each class\n",
    "## return format is list of dicts, annoying to work with. for k=1 it's just the max class, annoying for a col per class proba score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZqNMJgTM-zb"
   },
   "source": [
    "* Cao - maybe use only RBD domain ,  (+- mutation site?)\n",
    "\n",
    "\n",
    "* Could get WT prediction, and get **delta** of mutant's predicted score vs wt - then examine that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuInD2PVjT_j"
   },
   "source": [
    "* https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "\n",
    "\n",
    "* Eval related batch size stuff: https://discuss.huggingface.co/t/batch-size-for-trainer-predict/3374/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bE5fCdPrJVGH"
   },
   "outputs": [],
   "source": [
    "# CAO_TEST_DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/cao_escaper_targets_max.csv\"##\n",
    "# CAO_TEST_DATA_PATH = \"/kaggle/input/humvir-proteins/cao_escaper_targets_max.csv\"\n",
    "CAO_TEST_DATA_PATH = \"cao_escaper_targets_max.csv\"\n",
    "\n",
    "### all data, more redundnat - same seq repeated, can join with our res to save time.\n",
    "# CAO_full_TEST_DATA_PATH = \"/kaggle/input/humvir-proteins/cao_escaper_targets_all.csv\"\n",
    "CAO_full_TEST_DATA_PATH = \"cao_escaper_targets_all.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WyIXqfCDsN--"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3820\n",
      "0        1273\n",
      "1        1273\n",
      "2        1273\n",
      "3        1273\n",
      "4        1273\n",
      "         ... \n",
      "34673    1273\n",
      "34674    1273\n",
      "34675    1273\n",
      "34676    1273\n",
      "34677    1273\n",
      "Name: seq, Length: 34678, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>wildtype</th>\n",
       "      <th>mutation</th>\n",
       "      <th>mut_escape</th>\n",
       "      <th>seq</th>\n",
       "      <th>mut_escape_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34673</th>\n",
       "      <td>489</td>\n",
       "      <td>Y</td>\n",
       "      <td>R</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34674</th>\n",
       "      <td>489</td>\n",
       "      <td>Y</td>\n",
       "      <td>S</td>\n",
       "      <td>0.185581</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34675</th>\n",
       "      <td>489</td>\n",
       "      <td>Y</td>\n",
       "      <td>T</td>\n",
       "      <td>0.088192</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34676</th>\n",
       "      <td>489</td>\n",
       "      <td>Y</td>\n",
       "      <td>V</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34677</th>\n",
       "      <td>489</td>\n",
       "      <td>Y</td>\n",
       "      <td>W</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34678 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       site wildtype mutation  mut_escape  \\\n",
       "0       331        N        A    0.000000   \n",
       "1       331        N        C    0.000000   \n",
       "2       331        N        D    0.000000   \n",
       "3       331        N        E    0.000000   \n",
       "4       331        N        F    0.000000   \n",
       "...     ...      ...      ...         ...   \n",
       "34673   489        Y        R    0.001041   \n",
       "34674   489        Y        S    0.185581   \n",
       "34675   489        Y        T    0.088192   \n",
       "34676   489        Y        V    0.014363   \n",
       "34677   489        Y        W    0.000956   \n",
       "\n",
       "                                                     seq  mut_escape_class  \n",
       "0      MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 0  \n",
       "1      MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 0  \n",
       "2      MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 0  \n",
       "3      MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 0  \n",
       "4      MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 0  \n",
       "...                                                  ...               ...  \n",
       "34673  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 1  \n",
       "34674  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 1  \n",
       "34675  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 1  \n",
       "34676  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 1  \n",
       "34677  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...                 1  \n",
       "\n",
       "[34678 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cao = pd.read_csv(CAO_TEST_DATA_PATH)\n",
    "if FAST_RUN:\n",
    "  df_cao = df_cao.head(400)\n",
    "\n",
    "df_cao_full = pd.read_csv(CAO_full_TEST_DATA_PATH)\n",
    "print(df_cao_full[\"seq\"].nunique())\n",
    "print(df_cao_full[\"seq\"].str.len())\n",
    "display(df_cao_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WAPjYhQG_Fpp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook/esm2_t33_650M_UR50D'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w_LiqdXNls_1"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "#OLD\n",
    "def get_escaper_scores(df:pd.DataFrame,trained_model,base_model,seqColName=\"seq\",truncate_cao=False,\n",
    "                       binary_target_col = None):\n",
    "  if truncate_cao:\n",
    "  ## truncated version +-20:\n",
    "    cao_sequences = df[seqColName].str[300:550].tolist() # ONLY RBD domain\n",
    "    max_len = 550-300\n",
    "  else:\n",
    "    cao_sequences = df[seqColName].str[0:1024].tolist()\n",
    "    max_len = df[seqColName].str.len().max()+2\n",
    "\n",
    "  cao_test_seq = tokenizer(cao_sequences,  truncation=True\n",
    "                          ,max_length= min(max_len,1024)#1024\n",
    "                        #  max_length= 700 #600 #model_max_len,\n",
    "                         ,padding=True#True,\n",
    "                           # ,padding=True\n",
    "                           ,return_tensors=\"pt\")\n",
    "\n",
    "  cao_test_seq = Dataset.from_dict(cao_test_seq)\n",
    "  # cao_test_seq = accelerator.prepare(cao_test_seq)\n",
    "    ## get naive models preds - WHY are they similar to trained models ???\n",
    "  # \"\"\"\n",
    "  base_trainer = Trainer(base_model,tokenizer=tokenizer,args=training_args)\n",
    "  raw_pred, _, _ = base_trainer.predict(cao_test_seq)\n",
    "  # predictedLabelOnCompanyData = np.argmax(raw_pred, axis=1)\n",
    "  # df[\"base_pred_vir\"] = raw_pred[:,1]\n",
    "  df[\"base_pred_vir\"] = torch.nn.functional.softmax(torch.tensor(raw_pred), dim=1)[:,1]\n",
    "  # \"\"\"\n",
    "\n",
    "  ## preds with proper model\n",
    "  trainer = Trainer(model=trained_model,tokenizer=tokenizer,args=training_args) # added here...\n",
    "    ### https://discuss.huggingface.co/t/transform-logits-to-probabilities-doesnt-work/14792\n",
    "  logits, _, _ = trainer.predict(cao_test_seq)\n",
    "\n",
    "  # print(softmax(logits[:,1]))\n",
    "  # df[\"probabilities\"] = softmax(logits[:,1]) #(preds_output[0], axis=1)#probabilities\n",
    "\n",
    "  df[\"model_pred_vir\"] =  torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:,1]\n",
    "  df[\"model_pred_hum\"] =  torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:,0]\n",
    "  # df[\"sum\"] = (df[\"model_pred_hum\"]+df[\"model_pred_vir\"]).round(2) # adds up to 1\n",
    "  df[\"model_pred_hum_delta\"] = df[\"model_pred_hum\"].round(4)\n",
    "\n",
    "  ## subtract WT pred score - to get deltas. WT is most common (as it reoccurs)\n",
    "  mode_score = df[\"model_pred_hum\"].round(4).mode()[0]\n",
    "  df[\"model_pred_hum_delta\"] = df[\"model_pred_hum_delta\"] - mode_score\n",
    "  print(df.describe().round(3))\n",
    "\n",
    "  display(df.select_dtypes([\"number\",\"bool\"]).corr().round(3))\n",
    "\n",
    "  if binary_target_col is not None:\n",
    "    print(\"humVir trained model - human pred delta - ROCAUC:\",roc_auc_score(df[binary_target_col], df[\"model_pred_hum_delta\"]).round(4))\n",
    "    print(\"humVir trained model ROCAUC:\",roc_auc_score(df[binary_target_col], df[\"model_pred_hum\"]).round(4))\n",
    "    print(classification_report(df[binary_target_col],np.argmax(logits, axis=-1)))\n",
    "    print(\"Untrained vir model: ROCAUC\",roc_auc_score(df[binary_target_col], df[\"base_pred_vir\"]).round(4))\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "## newer, using pipeline (pretrained)\n",
    "## https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipeline-batching\n",
    "### pipeline approach here is MUCH slower! ?\n",
    "def pipe_get_escaper_scores(df:pd.DataFrame,trained_model,base_model,seqColName=\"seq\",truncate_cao=False,\n",
    "                       binary_target_col = None,model_checkpoint='facebook/esm2_t12_35M_UR50D'):\n",
    "    \"\"\"\n",
    "    Much slower than old hacky approach of using a trainer ...\n",
    "    \"\"\"\n",
    "    if truncate_cao:\n",
    "        ## truncated version +-20:\n",
    "        cao_sequences = df[seqColName].str[300:550].tolist() # ONLY RBD domain\n",
    "        max_len = 550-300\n",
    "    else:\n",
    "        cao_sequences = df[seqColName].str[0:1024].tolist()\n",
    "        max_len = df[seqColName].str.len().max()+2\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,padding= True ,truncation=True,max_length=min(max_len,1024),is_split_into_words=False)\n",
    "\n",
    "    def do_tokenization(example):\n",
    "        \"\"\"\n",
    "        use like\n",
    "        dataset = dataset.map(do_tokenization, batched=True)\n",
    "        \"\"\"\n",
    "        return tokenizer(example[\"text\"])\n",
    "\n",
    "    # cao_sequences = tokenizer(cao_sequences, truncation=True,max_length=model_max_len,is_split_into_words=False) ## ,return_tensors='pt'\n",
    "\n",
    "    # cao_sequences = Dataset.from_dict(cao_sequences)\n",
    "    # encoded_dataset = dataset.map(lambda examples: tokenizer(examples['sentence1']),\n",
    "    #                           batched=True)\n",
    "    #\n",
    "    # cao_sequences = Dataset.from_list(tokenizer(cao_sequences,  truncation=True,max_length=model_max_len,padding=False))#.map(tokenizer, batched=True)\n",
    "    # cao_sequences = Dataset.from_pandas(pd.DataFrame(cao_sequences)).map(tokenizer, batched=True)\n",
    "    # cao_sequences = Dataset.from_pandas(df[seqColName])\n",
    "\n",
    "    # cao_test_seq = tokenizer(cao_sequences,  truncation=True\n",
    "    #                       ,max_length= min(max_len,1024)#1024\n",
    "    #                     #  max_length= 700 #600 #model_max_len,\n",
    "    #                      ,padding=False#True,\n",
    "    #                        # ,padding=True\n",
    "    #                        ,return_tensors=\"pt\")\n",
    "\n",
    "    # cao_test_seq = Dataset.from_dict(cao_test_seq)\n",
    "    # # cao_test_seq = accelerator.prepare(cao_test_seq)\n",
    "    # ## get naive models preds - WHY are they similar to trained models ???\n",
    "\n",
    "    def set_vir_label_scores(row):\n",
    "        \"\"\"\n",
    "        Get the virus score per row, (and we can externally set human score from that).\n",
    "        Assumes model_score col is the k=1 / top score\n",
    "        \"\"\"\n",
    "        if row.model_label.lower() == \"virus\":\n",
    "            return row.model_score\n",
    "        else:\n",
    "            return 1-row.model_score\n",
    "\n",
    "\n",
    "    ### naive model - should be random res\n",
    "    naive_classifier_pipe = pipeline(\"text-classification\",model=base_model, tokenizer=tokenizer,device=0, top_k=1,batch_size=16) # ,device_map=\"auto\"\n",
    "    # naive_classifier_pipe = pipeline(\"text-classification\",model=base_model, tokenizer=tokenizer, top_k=1,device=0)\n",
    "    naive_preds = naive_classifier_pipe(cao_sequences)\n",
    "    naive_preds_labels = [i[0][\"label\"] for i in naive_preds]\n",
    "    naive_preds_top_score = [i[0][\"score\"] for i in naive_preds]\n",
    "    df[\"model_label\"] = naive_preds_labels\n",
    "    df[\"model_label\"] = df[\"model_label\"].str.lower()\n",
    "    df[\"model_score\"] = naive_preds_top_score\n",
    "\n",
    "    df[\"base_pred_vir\"] = df.apply(set_vir_label_scores, axis=1)\n",
    "    df.drop(columns=[\"model_label\",\"model_score\"],inplace=True,errors=\"ignore\")\n",
    "\n",
    "    ## get proepr trained models predictions\n",
    "    pipe = pipeline(\"text-classification\",model=trained_model, tokenizer=tokenizer, top_k=1,batch_size=16,device=0)  # ,truncation=True\n",
    "    # pipe = pipeline(\"text-classification\",model=trained_model, tokenizer=tokenizer, top_k=1,device=0)\n",
    "    ## pipeline (with k=1) returns label, score for top label, need to get per lavbel\n",
    "    preds = pipe(cao_sequences)\n",
    "    preds_labels = [i[0][\"label\"] for i in preds]\n",
    "    preds_top_score = [i[0][\"score\"] for i in preds]\n",
    "\n",
    "    df[\"model_label\"] = preds_labels\n",
    "    df[\"model_label\"] = df[\"model_label\"].str.lower()\n",
    "    df[\"model_score\"] = preds_top_score\n",
    "\n",
    "    df[\"model_pred_vir\"] = df.apply(set_vir_label_scores, axis=1)\n",
    "    df[\"model_pred_hum\"] = 1-df[\"model_pred_vir\"]\n",
    "    df[\"model_pred_hum_delta\"] = df[\"model_pred_hum\"].round(4)\n",
    "\n",
    "    ## subtract WT pred score - to get deltas. WT is most common (as it reoccurs)\n",
    "    mode_score = df[\"model_pred_hum\"].round(4).mode()[0]\n",
    "    df[\"model_pred_hum_delta\"] = df[\"model_pred_hum_delta\"] - mode_score\n",
    "    print(df.describe().round(3))\n",
    "\n",
    "    display(df.select_dtypes([\"number\",\"bool\"]).corr().round(3))\n",
    "\n",
    "    if binary_target_col is not None:\n",
    "        print(\"humVir trained model - human pred delta - ROCAUC:\",roc_auc_score(df[binary_target_col], df[\"model_pred_hum_delta\"]).round(4))\n",
    "        print(\"humVir trained model ROCAUC:\",roc_auc_score(df[binary_target_col], df[\"model_pred_hum\"]).round(4))\n",
    "        # print(classification_report(df[binary_target_col],np.argmax(logits, axis=-1)))\n",
    "        print(classification_report(df[binary_target_col],df[\"model_label\"]==1))\n",
    "        print(\"Untrained vir model: ROCAUC\",roc_auc_score(df[binary_target_col], df[\"base_pred_vir\"]).round(4))\n",
    "\n",
    "    return df\n",
    "\n",
    "# get_escaper_scores(df_iedb_vir.sample(16).copy(),trained_model=model,base_model=base_model,seqColName=\"Epitopes - Epitope\",truncate_cao=False)\n",
    "\n",
    "# pipe_get_escaper_scores(df_cao.head(120).copy(),trained_model= merged_model#trainer.model,\n",
    "#                    ,base_model=base_model,seqColName=\"seq\",truncate_cao=True,\n",
    "#                        binary_target_col = \"mut_escape_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Nx94FfFvJtoo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>wildtype</th>\n",
       "      <th>mutation</th>\n",
       "      <th>mut_escape</th>\n",
       "      <th>mut_escape_class</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>0.112258</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>0.012256</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>V</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>W</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4020 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      site wildtype mutation  mut_escape  mut_escape_class  \\\n",
       "0      331        N        A    0.009428                 1   \n",
       "1      331        N        C    0.000000                 0   \n",
       "2      331        N        D    0.026596                 1   \n",
       "3      331        N        E    0.112258                 1   \n",
       "4      331        N        F    0.012256                 1   \n",
       "...    ...      ...      ...         ...               ...   \n",
       "4015   531        T        S    0.010178                 1   \n",
       "4016   531        T        T    0.000000                 0   \n",
       "4017   531        T        V    0.009521                 1   \n",
       "4018   531        T        W    0.008336                 1   \n",
       "4019   531        T        Y    0.017168                 1   \n",
       "\n",
       "                                                    seq  \n",
       "0     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "1     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "2     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "3     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "...                                                 ...  \n",
       "4015  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4016  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4017  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4018  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4019  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "\n",
       "[4020 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SH0IzxxHkYVv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### 12 sec\n",
    "# df_cao_preds = get_escaper_scores(df_cao.copy(),trained_model= merged_model#model, #trainer,\n",
    "#                    ,base_model=base_model,seqColName=\"seq\",truncate_cao=True,\n",
    "#                        binary_target_col = \"mut_escape_class\")\n",
    "\n",
    "# ## fast run. NOTE! trained model res are different than when using pipe!... !\n",
    "# # humVir trained model - human pred delta - ROCAUC: 0.4858\n",
    "# # humVir trained model ROCAUC: 0.4858\n",
    "# # Untrained vir model: ROCAUC 0.4922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ACslPC3Ah8Ey",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ###  slow!\n",
    "# df_cao_preds = pipe_get_escaper_scores(df_cao.copy(),trained_model= merged_model#trainer.model,\n",
    "#                    ,base_model=base_model,seqColName=\"seq\",truncate_cao=True,\n",
    "#                        binary_target_col = \"mut_escape_class\")\n",
    "\n",
    "# ### 1 min for all\n",
    "# ## fast run. NOTE! trained model res are different than when not using pipe!... !\n",
    "# # humVir trained model - human pred delta - ROCAUC: 0.4798\n",
    "# # humVir trained model ROCAUC: 0.4798\n",
    "# # Untrained vir model: ROCAUC 0.4922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "XlIyuv_n_Fpq"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "### works but very slow\n",
    "# # model_saved(**inputs)\n",
    "# samples = df_cao.head(70)[\"seq\"].to_list()\n",
    "# merged_model(**tokenizer(samples,truncation=True,return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_F4FYloQ4A5R"
   },
   "outputs": [],
   "source": [
    "# df_cao[\"model_pred_vir\"].describe().round(4)\n",
    "# df_cao[\"model_pred_hum\"].value_counts() ## most common (271) value - unmutated score/WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "cTc1aPqhSreR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>mut_escape</th>\n",
       "      <th>mut_escape_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mut_escape</th>\n",
       "      <td>0.073</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mut_escape_class</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.555</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   site  mut_escape  mut_escape_class\n",
       "site              1.000       0.073             0.093\n",
       "mut_escape        0.073       1.000             0.555\n",
       "mut_escape_class  0.093       0.555             1.000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cao.select_dtypes([\"number\",\"bool\"]).corr().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "NAaFUX9C_Fpq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3820"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## merge preds and plot vs escaper score (across many conditions. partially dedupped by cond + identicals)\n",
    "df_cao_full[\"seq\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FTGZPcihcXw"
   },
   "source": [
    "### try FT on CAO\n",
    "* Split by site?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "FYXFYyJ1hZsk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>wildtype</th>\n",
       "      <th>mutation</th>\n",
       "      <th>mut_escape</th>\n",
       "      <th>mut_escape_class</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>0.112258</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>331</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>0.012256</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>V</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>W</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>531</td>\n",
       "      <td>T</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>1</td>\n",
       "      <td>MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4020 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      site wildtype mutation  mut_escape  mut_escape_class  \\\n",
       "0      331        N        A    0.009428                 1   \n",
       "1      331        N        C    0.000000                 0   \n",
       "2      331        N        D    0.026596                 1   \n",
       "3      331        N        E    0.112258                 1   \n",
       "4      331        N        F    0.012256                 1   \n",
       "...    ...      ...      ...         ...               ...   \n",
       "4015   531        T        S    0.010178                 1   \n",
       "4016   531        T        T    0.000000                 0   \n",
       "4017   531        T        V    0.009521                 1   \n",
       "4018   531        T        W    0.008336                 1   \n",
       "4019   531        T        Y    0.017168                 1   \n",
       "\n",
       "                                                    seq  \n",
       "0     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "1     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "2     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "3     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4     MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "...                                                 ...  \n",
       "4015  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4016  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4017  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4018  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "4019  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSS...  \n",
       "\n",
       "[4020 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "asEjhb6-htcz"
   },
   "outputs": [],
   "source": [
    "tokenizer_cao = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                          padding= True\n",
    "                                          ,truncation=True,max_length= 552#252 ## [300:550]\n",
    "                                               #df_cao[\"seq\"].str.len().max()\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFQUzEgRj58m"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhGrG6_mj533"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KeMKAN2Xhtcz"
   },
   "outputs": [],
   "source": [
    "train_sequences,test_sequences,train_labels,test_labels = train_test_split(df_cao[\"seq\"].str[0:552],df_cao[\"mut_escape_class\"],\n",
    "                                                                           stratify=df_cao[\"mut_escape_class\"],test_size=0.25)\n",
    "\n",
    "train_dataset_immuno_all,test_dataset_immuno = get_train_test_dataseqs(train_sequences.to_list(),test_sequences.to_list(),\n",
    "                                                                   train_labels,test_labels,tokenizer_cao)\n",
    "\n",
    "ds_splits = train_dataset_immuno_all.train_test_split(test_size=0.1,seed =1)\n",
    "# train_dataset_immuno, eval_dataset_immuno = train_dataset_immuno.train_test_split(test_size=0.1)\n",
    "eval_dataset_immuno = ds_splits[\"test\"]\n",
    "train_dataset_immuno = ds_splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3Rg-3XcNhtcz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 302\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "OhOA6hODhtc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2713\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-HpjRolGhtc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2713\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "TJMPElUghtc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mddofer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403302b677794211952b82e391ed8660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112403222230366, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/New Protein-Virus anom project/wandb/run-20240311_230659-2907sf5x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ddofer/huggingface/runs/2907sf5x' target=\"_blank\">resilient-star-58</a></strong> to <a href='https://wandb.ai/ddofer/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ddofer/huggingface' target=\"_blank\">https://wandb.ai/ddofer/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ddofer/huggingface/runs/2907sf5x' target=\"_blank\">https://wandb.ai/ddofer/huggingface/runs/2907sf5x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1020 07:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690268</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.271523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.561528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.718900</td>\n",
       "      <td>0.689738</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.271523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.495758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.694400</td>\n",
       "      <td>0.690747</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.271523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.474174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Checkpoint destination directory esm2_t33_650M_UR50D-cao_ft/checkpoint-340 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Checkpoint destination directory esm2_t33_650M_UR50D-cao_ft/checkpoint-680 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Checkpoint destination directory esm2_t33_650M_UR50D-cao_ft/checkpoint-1020 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args_cao = TrainingArguments(\n",
    "    # f\"{model_name}-finetuned-ft-humVir\",\n",
    "     f\"{model_name}-cao_ft\",\n",
    "    overwrite_output_dir = True,\n",
    "    # num_train_epochs=1,\n",
    "    # f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "    # per_device_train_batch_size=16,\n",
    "    # per_device_eval_batch_size=int(1.5*bch_size),\n",
    "#     gradient_accumulation_steps= 2, #4,\n",
    "    gradient_checkpointing=True,\n",
    "    # fp16=True,\n",
    "    bf16=True, # needs ampere, not supported ?\n",
    "    tf32=True,\n",
    "        # torch_compile = True,\n",
    "    optim =  \"paged_adamw_32bit\", # \"adamw_8bit\", #\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "    # optim= \"adamw_bnb_8bit\", #\"paged_adamw_8bit\",\n",
    "    label_names = [\"labels\"],\n",
    "    learning_rate = 4e-4,#2e-4 #5e-3,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # max_grad_norm = 0.95,\n",
    "    # eval_accumulation_steps = 2#8\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy= \"epoch\", #\"epoch\"#\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    # weight_decay = 0.001,\n",
    "    # output_dir=\".\",\n",
    "     no_cuda=False,\n",
    "     greater_is_better=True,\n",
    "     # save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "    # auto_find_batch_size = True, # new , reduces if oom\n",
    "    # num_train_epochs=num_epochs,\n",
    "    metric_for_best_model= \"eval_roc_auc\",#\"accuracy\",\n",
    "    # group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer_immuno_iedb = Trainer(model=merged_model, args=training_args_cao,\n",
    "                  train_dataset=train_dataset_immuno,eval_dataset=eval_dataset_immuno,tokenizer=tokenizer_cao,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "result_immuno_iedb = trainer_immuno_iedb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "zMsqYLBFhtc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6919892430305481,\n",
       " 'eval_accuracy': 0.5283582089552239,\n",
       " 'eval_precision': 0.26417910447761195,\n",
       " 'eval_recall': 0.5,\n",
       " 'eval_roc_auc': 0.503168530040446,\n",
       " 'eval_runtime': 14.4993,\n",
       " 'eval_samples_per_second': 69.314,\n",
       " 'eval_steps_per_second': 8.69,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_immuno_iedb.evaluate(test_dataset_immuno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "YKBFAPWhhtc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1020 14:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691471</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.271523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.496023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.691018</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.271523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.690268</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.271523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Checkpoint destination directory cao_base/checkpoint-340 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Checkpoint destination directory cao_base/checkpoint-680 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Checkpoint destination directory cao_base/checkpoint-1020 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6915850639343262,\n",
       " 'eval_accuracy': 0.5283582089552239,\n",
       " 'eval_precision': 0.26417910447761195,\n",
       " 'eval_recall': 0.5,\n",
       " 'eval_roc_auc': 0.5388725992673644,\n",
       " 'eval_runtime': 29.5186,\n",
       " 'eval_samples_per_second': 34.046,\n",
       " 'eval_steps_per_second': 4.268,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_cao.output_dir=\"cao_base\"\n",
    "trainer_immuno_iedb_base = Trainer(model=base_model, args=training_args_cao,\n",
    "                  train_dataset=train_dataset_immuno,eval_dataset=eval_dataset_immuno,tokenizer=tokenizer_cao,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "result_immuno_iedb_base = trainer_immuno_iedb_base.train()\n",
    "\n",
    "trainer_immuno_iedb_base.evaluate(test_dataset_immuno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiO5IxnQj2_D"
   },
   "source": [
    "Viral escape data:\n",
    "\n",
    "```\n",
    "!wget http://cb.csail.mit.edu/cb/viral-mutation/data.tar.gz\n",
    "!tar xvf data.tar.gz\n",
    "```\n",
    "* https://github.com/brianhie/viral-mutation/tree/master/results\n",
    "* wget from : https://colab.research.google.com/github/vanvalenlab/bebi205/blob/master/bebi205/notebooks/sequences-key.ipynb#scrollTo=VN607f41e_qp\n",
    "  * relatively large - 3GB zipped!\n",
    "\n",
    "\n",
    "Example output from them (maybe cov2 seq), with escaper results value:\n",
    "https://github.com/brianhie/viral-mutation/blob/master/examples/example_results.txt\n",
    "\n",
    "* another for flu (1, h3)... https://github.com/brianhie/viral-mutation/blob/master/results/flu/semantics/analyze_semantics_flu_h1_bilstm_512.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "oQqyQiIokI5I"
   },
   "outputs": [],
   "source": [
    "# !wget http://cb.csail.mit.edu/cb/viral-mutation/data.tar.gz\n",
    "# !tar xvf data.tar.gz\n",
    "### https://github.com/brianhie/viral-mutation/blob/81c80d41671670eb58cc46e957a1b0c4bf14856a/bin/escape.py#L5\n",
    "# # pd.read_csv('data/influenza/escape_doud2018/pos_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5mfHmXg0kJjV"
   },
   "outputs": [],
   "source": [
    "# df_ex = pd.read_csv(\"https://raw.githubusercontent.com/brianhie/viral-mutation/master/examples/example_results.txt\",sep=\"\\t\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "zrDjcxEXlkuv"
   },
   "outputs": [],
   "source": [
    "# df_ex = get_escaper_scores(df_ex,trained_model=merged_model,base_model=base_model,seqColName=\"Sequence\",truncate_cao=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLLDDl_2_Fpt"
   },
   "source": [
    "#### DeepImmuno train data\n",
    "* https://github.com/frankligy/DeepImmuno/blob/main/reproduce/data/remove0123_sample100.csv\n",
    "*  this is the train set from there\n",
    "*  * Deep-learning empowered prediction and generation of immunogenic epitopes for T cell immunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "HOvv4mwI_Fpt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>HLA</th>\n",
       "      <th>immunogenicity</th>\n",
       "      <th>test</th>\n",
       "      <th>respond</th>\n",
       "      <th>potential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KEHVFFSEY</td>\n",
       "      <td>HLA-B*4402</td>\n",
       "      <td>Negative</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.347444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEGATLYRF</td>\n",
       "      <td>HLA-B*4402</td>\n",
       "      <td>Negative</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.346545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TLAARIKFL</td>\n",
       "      <td>HLA-A*0201</td>\n",
       "      <td>Negative</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.346239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KETLNEYKQL</td>\n",
       "      <td>HLA-B*4402</td>\n",
       "      <td>Negative</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.345162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STTDAEACY</td>\n",
       "      <td>HLA-A*0101</td>\n",
       "      <td>Negative</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8966</th>\n",
       "      <td>YYMATLKNV</td>\n",
       "      <td>HLA-A*2402</td>\n",
       "      <td>Positive</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>ILMNDQEVGV</td>\n",
       "      <td>HLA-A*0201</td>\n",
       "      <td>Positive</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>0.356673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8968</th>\n",
       "      <td>ALYEKKLAL</td>\n",
       "      <td>HLA-A*0201</td>\n",
       "      <td>Positive</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8969</th>\n",
       "      <td>GIIYIIYKL</td>\n",
       "      <td>HLA-A*0201</td>\n",
       "      <td>Positive</td>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>0.280177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8970</th>\n",
       "      <td>YGDDVIASY</td>\n",
       "      <td>HLA-A*0101</td>\n",
       "      <td>Positive</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>0.262157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8971 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         peptide         HLA immunogenicity  test  respond  potential\n",
       "0      KEHVFFSEY  HLA-B*4402       Negative     4        0   0.347444\n",
       "1      DEGATLYRF  HLA-B*4402       Negative     4        0   0.346545\n",
       "2      TLAARIKFL  HLA-A*0201       Negative     4        0   0.346239\n",
       "3     KETLNEYKQL  HLA-B*4402       Negative     4        0   0.345162\n",
       "4      STTDAEACY  HLA-A*0101       Negative     4        0   0.343674\n",
       "...          ...         ...            ...   ...      ...        ...\n",
       "8966   YYMATLKNV  HLA-A*2402       Positive    50        1   0.370625\n",
       "8967  ILMNDQEVGV  HLA-A*0201       Positive    89       13   0.356673\n",
       "8968   ALYEKKLAL  HLA-A*0201       Positive    80        3   0.300920\n",
       "8969   GIIYIIYKL  HLA-A*0201       Positive    89        4   0.280177\n",
       "8970   YGDDVIASY  HLA-A*0101       Positive    89        2   0.262157\n",
       "\n",
       "[8971 rows x 6 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immuno_train = pd.read_csv(\"DeepImmuno_train_remove0123_sample100.csv\").drop_duplicates(subset=[\"peptide\",\"HLA\",\"immunogenicity\"])\n",
    "df_immuno_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "aOqnIohE_Fpt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peptide           8383\n",
       "HLA                104\n",
       "immunogenicity       5\n",
       "test                64\n",
       "respond             44\n",
       "potential         8970\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immuno_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "m5uDMOZx_Fpt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "immunogenicity\n",
       "Negative                 4912\n",
       "Positive                 3747\n",
       "Positive-Low              187\n",
       "Positive-High              87\n",
       "Positive-Intermediate      38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immuno_train.immunogenicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "zdiw3nbE_Fpt"
   },
   "outputs": [],
   "source": [
    "df_immuno_train[\"pos_immunogen\"] = df_immuno_train[\"immunogenicity\"].str.contains(\"Positive\",case=False,na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "qARjv3P-_Fpu"
   },
   "outputs": [],
   "source": [
    "# df_immuno_train_res = get_escaper_scores(df_immuno_train,trained_model=merged_model,base_model=base_model,seqColName=\"peptide\",truncate_cao=False,binary_target_col=\"pos_immunogen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "4-RcnpVt_Fpu"
   },
   "outputs": [],
   "source": [
    "# df_immuno_train_res[['potential',\n",
    "#        'pos_immunogen', 'base_pred_vir', 'model_pred_vir', 'model_pred_hum',\n",
    "#        'model_pred_hum_delta']].corr().round(3)#.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k98V_ta2_Fpu"
   },
   "source": [
    "### Try with fine tuning - deepimmuno\n",
    "* compare also to baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "A5FBo3a3_Fpu"
   },
   "outputs": [],
   "source": [
    "tokenizer_immuno = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                          padding= True\n",
    "                                          ,truncation=True,max_length=df_immuno_train[\"peptide\"].str.len().max().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immuno_train[\"peptide\"].str.len().max().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "2I92Z0i2_Fpu"
   },
   "outputs": [],
   "source": [
    "train_sequences,test_sequences,train_labels,test_labels = train_test_split(df_immuno_train[\"peptide\"],df_immuno_train[\"pos_immunogen\"],\n",
    "                                                                        stratify=df_immuno_train[\"pos_immunogen\"],test_size=0.35)\n",
    "\n",
    "train_dataset_immuno_all,test_dataset_immuno = get_train_test_dataseqs(train_sequences.to_list(),test_sequences.to_list(),\n",
    "                                                                   train_labels,test_labels,tokenizer_immuno)\n",
    "\n",
    "ds_splits = train_dataset_immuno_all.train_test_split(test_size=0.1,seed =1)\n",
    "# train_dataset_immuno, eval_dataset_immuno = train_dataset_immuno.train_test_split(test_size=0.1)\n",
    "eval_dataset_immuno = ds_splits[\"test\"]\n",
    "train_dataset_immuno = ds_splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9AfHD-1p_Fpu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 584\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_MexckxQ_Fpu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5247\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "gCIyHDsa_Fpv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5247\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "MfLWTpf6_Fpv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='329' max='984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/984 00:45 < 01:31, 7.14 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.552209</td>\n",
       "      <td>0.725159</td>\n",
       "      <td>0.722771</td>\n",
       "      <td>0.723302</td>\n",
       "      <td>0.787787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 39\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args_immuno \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# f\"{model_name}-finetuned-ft-humVir\",\u001b[39;00m\n\u001b[1;32m      3\u001b[0m      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-deepImmuno_ft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# group_by_length=True,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m trainer_immuno \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmerged_model, args\u001b[38;5;241m=\u001b[39mtraining_args_immuno,\n\u001b[1;32m     37\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset_immuno,eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset_immuno,tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_immuno,\n\u001b[1;32m     38\u001b[0m                   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,)\n\u001b[0;32m---> 39\u001b[0m result_immuno \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_immuno\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2049\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2049\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2053\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2423\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2499\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:3016\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:3094\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\n\u001b[1;32m   3090\u001b[0m         output_dir, state_dict\u001b[38;5;241m=\u001b[39mstate_dict, safe_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors\n\u001b[1;32m   3091\u001b[0m     )\n\u001b[1;32m   3093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;66;03m# Good practice: save your training arguments together with the trained model\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, TRAINING_ARGS_NAME))\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2457\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.save_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     tokenizer_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokenizer_config_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 2457\u001b[0m     out_str \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2458\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(out_str)\n\u001b[1;32m   2459\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer config file saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "training_args_immuno = TrainingArguments(\n",
    "    # f\"{model_name}-finetuned-ft-humVir\",\n",
    "     f\"{model_name}-deepImmuno_ft\",\n",
    "     overwrite_output_dir = True,\n",
    "    # num_train_epochs=4,\n",
    "    # f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "    per_device_train_batch_size=16,\n",
    "    # per_device_eval_batch_size=int(1.5*bch_size),\n",
    "#     gradient_accumulation_steps= 2, #4,\n",
    "    gradient_checkpointing=True,\n",
    "    # fp16=True,\n",
    "    bf16=True, # needs ampere, not supported ?\n",
    "    tf32=True,\n",
    "        # torch_compile = True,\n",
    "    # optim = \"adamw_8bit\", #\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "    # optim= \"adamw_bnb_8bit\", #\"paged_adamw_8bit\",\n",
    "    label_names = [\"labels\"],\n",
    "    learning_rate = 3e-4,#2e-4 #5e-3,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # ,max_grad_norm = 0.95,\n",
    "    # eval_accumulation_steps = 2#8\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy= \"epoch\", #\"no\", #\"epoch\"#\"no\",\n",
    "    # output_dir=\".\",\n",
    "     no_cuda=False,\n",
    "     greater_is_better=True,\n",
    "     # save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "    # auto_find_batch_size = True, # new , reduces if oom\n",
    "    # num_train_epochs=num_epochs,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model= \"eval_roc_auc\",#\"accuracy\",\n",
    "    # group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer_immuno = Trainer(model=merged_model, args=training_args_immuno,\n",
    "                  train_dataset=train_dataset_immuno,eval_dataset=eval_dataset_immuno,tokenizer=tokenizer_immuno,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "result_immuno = trainer_immuno.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Qnondsd1_Fpv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5522086024284363,\n",
       " 'eval_accuracy': 0.7251592356687898,\n",
       " 'eval_precision': 0.7227712622666138,\n",
       " 'eval_recall': 0.7233017657926744,\n",
       " 'eval_roc_auc': 0.7877871976858386}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_immuno.evaluate(test_dataset_immuno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "cMXrtXhD_Fpv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='329' max='984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/984 00:51 < 01:43, 6.35 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580270</td>\n",
       "      <td>0.714041</td>\n",
       "      <td>0.722424</td>\n",
       "      <td>0.701459</td>\n",
       "      <td>0.789024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./esm_lora_trainer_model_sml - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args_immuno\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmuno_base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m trainer_immuno_base \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mbase_model, args\u001b[38;5;241m=\u001b[39mtraining_args_immuno,\n\u001b[1;32m      3\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset_immuno,eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset_immuno,tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_immuno,\n\u001b[1;32m      4\u001b[0m                   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,)\n\u001b[0;32m----> 5\u001b[0m result_immuno_base \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_immuno_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2049\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2049\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2053\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2423\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2499\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:3016\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:3094\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\n\u001b[1;32m   3090\u001b[0m         output_dir, state_dict\u001b[38;5;241m=\u001b[39mstate_dict, safe_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors\n\u001b[1;32m   3091\u001b[0m     )\n\u001b[1;32m   3093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;66;03m# Good practice: save your training arguments together with the trained model\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, TRAINING_ARGS_NAME))\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2457\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.save_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     tokenizer_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokenizer_config_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 2457\u001b[0m     out_str \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2458\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(out_str)\n\u001b[1;32m   2459\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer config file saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "training_args_immuno.output_dir=\"immuno_base\"\n",
    "trainer_immuno_base = Trainer(model=base_model, args=training_args_immuno,\n",
    "                  train_dataset=train_dataset_immuno,eval_dataset=eval_dataset_immuno,tokenizer=tokenizer_immuno,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "result_immuno_base = trainer_immuno_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtAACOkM_Fpv"
   },
   "outputs": [],
   "source": [
    "trainer_immuno_base.evaluate(test_dataset_immuno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5lDwY24_Fpv"
   },
   "source": [
    "### IEDB epitopes\n",
    "*lots of (short) linear epitopes\n",
    "* humans, viruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7zJlWjr_Fpw"
   },
   "outputs": [],
   "source": [
    "df_iedb_human = pd.read_csv(\"iedb_human_epitopes.csv\",compression=\"zip\").drop_duplicates(subset=[\"Epitopes - Epitope\"])\n",
    "## raw - 1.1M seqs\n",
    "print(df_iedb_human.nunique())\n",
    "df_iedb_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npH_4j4q_Fpw"
   },
   "outputs": [],
   "source": [
    "df_iedb_human[['Epitopes - # References','Epitopes - # Assays']].max(axis=1).describe().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCN9q-Hg_Fpw"
   },
   "source": [
    "### filter for higher confidence cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuXC6c7y_Fpw"
   },
   "outputs": [],
   "source": [
    "df_iedb_human = df_iedb_human.loc[df_iedb_human[['Epitopes - # References','Epitopes - # Assays']].max(axis=1)>1]\n",
    "print(df_iedb_human.shape[0])\n",
    "df_iedb_human = df_iedb_human.loc[~df_iedb_human[\"Epitopes - Epitope\"].str.contains(\"[\\+ ]\")]\n",
    "print(df_iedb_human.shape[0])\n",
    "\n",
    "df_iedb_human = df_iedb_human.loc[df_iedb_human[\"Epitopes - Epitope\"].str.len()>4]\n",
    "\n",
    "df_iedb_human[\"labels\"] = 0\n",
    "print(df_iedb_human.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb9-Myy8_Fpw"
   },
   "outputs": [],
   "source": [
    "df_iedb_human[\"Epitopes - Epitope\"].str.len().describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGexsQHB_Fpw"
   },
   "outputs": [],
   "source": [
    "df_iedb_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "js_4JUCg_Fpx"
   },
   "outputs": [],
   "source": [
    "df_iedb_human[\"Epitopes - Epitope\"].str.len().describe().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL7Ot9WR_Fpx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_iedb_human_preds = get_escaper_scores(df_iedb_human.sample(40_123,).copy(),trained_model=merged_model # model,\n",
    "                                         ,base_model=base_model,seqColName=\"Epitopes - Epitope\",truncate_cao=False)\n",
    "df_iedb_human_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xQu9OW8_Fpx"
   },
   "outputs": [],
   "source": [
    "df_iedb_human_preds[\"model_pred_hum\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMgcjte3_Fpx"
   },
   "outputs": [],
   "source": [
    "# \"E:\\Stuff\\Research\\datasets\\New Protein-Virus anom project\\iedb_vir_epitopes.csv.zip.csv\"\n",
    "df_iedb_vir = pd.read_csv(\"iedb_vir_epitopes.csv\",compression=\"zip\").drop_duplicates(subset=[\"Epitopes - Epitope\"])\n",
    "## raw - 138K seqs\n",
    "print(df_iedb_vir.nunique())\n",
    "# df_iedb_vir\n",
    "\n",
    "df_iedb_vir = df_iedb_vir.loc[df_iedb_vir[['Epitopes - # References','Epitopes - # Assays']].max(axis=1)>1]\n",
    "print(df_iedb_vir.shape[0])\n",
    "df_iedb_vir = df_iedb_vir.loc[~df_iedb_vir[\"Epitopes - Epitope\"].str.contains(\"[\\+ ]\")]\n",
    "print(df_iedb_vir.shape[0])\n",
    "\n",
    "df_iedb_vir = df_iedb_vir.loc[df_iedb_vir[\"Epitopes - Epitope\"].str.len()>5]\n",
    "\n",
    "df_iedb_vir[\"labels\"] = 1\n",
    "print(df_iedb_vir.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dZZPrP__Fpx"
   },
   "outputs": [],
   "source": [
    "df_iedb_vir[\"Epitopes - Epitope\"].str.len().describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvZF46DJ_Fpx"
   },
   "outputs": [],
   "source": [
    "df_iedb_vir_preds = get_escaper_scores(df_iedb_vir.sample(40_183).copy(),trained_model=merged_model,base_model=base_model,seqColName=\"Epitopes - Epitope\",truncate_cao=False)\n",
    "df_iedb_vir_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pezSYnoF_Fpy"
   },
   "outputs": [],
   "source": [
    "df_iedb_vir_preds[\"model_pred_hum\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmED6RFk_Fpy"
   },
   "outputs": [],
   "source": [
    "df_iedb_vir_preds[\"label\"]=\"vir\"\n",
    "df_iedb_human_preds[\"label\"]=\"hum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDUpxC_Z_Fp0"
   },
   "outputs": [],
   "source": [
    "df_iedbs = pd.concat([df_iedb_vir_preds[[\"label\",\"model_pred_hum\"]],df_iedb_human_preds[[\"label\",\"model_pred_hum\"]]])\n",
    "df_iedbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5nGxsNY_Fp0"
   },
   "outputs": [],
   "source": [
    "### FIG 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkS6nhFL_Fp1"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# https://stackoverflow.com/questions/28293028/plotting-grouped-data-in-same-plot-using-pandas\n",
    "# df_iedbs.groupby([\"label\"]).plot(kind='kde', ax=plt.gca())\n",
    "\n",
    "# sns.kdeplot(data=df_iedbs, x=\"model_pred_hum\", hue=\"label\")#, bw_adjust=1, cut=1)\n",
    "df2 = df_iedbs.copy()\n",
    "df2.rename(columns={\"model_pred_hum\":\"Predicted Humanness score\"},inplace=True)\n",
    "df2[\"label\"] = df2[\"label\"].str.replace(\"vir\",\"Virus\").str.replace(\"hum\",\"Human\")\n",
    "sns.kdeplot(data=df2, x=\"Predicted Humanness score\", hue=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6mnS4KU_Fp1"
   },
   "outputs": [],
   "source": [
    "df_iedbs.groupby(\"label\").describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wdpVkWO_Fp1"
   },
   "source": [
    "### Try FT on iedb peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qotFBTW2_Fp1"
   },
   "outputs": [],
   "source": [
    "df_iedbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWpnRDs5_Fp1"
   },
   "source": [
    "### Try with fine tuning - deepimmuno\n",
    "* compare also to baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yd8liesj_Fp1"
   },
   "outputs": [],
   "source": [
    "df_iedb_sample = pd.concat([df_iedb_vir.sample(35_000),df_iedb_human.sample(35_000)]).filter([\"Epitopes - Epitope\",\"labels\"])\n",
    "\n",
    "df_iedb_sample.rename(columns={\"Epitopes - Epitope\":\"peptide\"},inplace=True)\n",
    "df_iedb_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUvktdL3_Fp1"
   },
   "outputs": [],
   "source": [
    "df_iedb_sample.peptide.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHZVhyy__Fp2"
   },
   "outputs": [],
   "source": [
    "tokenizer_iedb = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                          padding= True\n",
    "                                          ,truncation=True,max_length=df_iedb_sample[\"peptide\"].str.len().max().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYRctS0F_Fp2"
   },
   "outputs": [],
   "source": [
    "train_sequences,test_sequences,train_labels,test_labels = train_test_split(df_iedb_sample[\"peptide\"],df_iedb_sample[\"labels\"],\n",
    "                                                                           stratify=df_iedb_sample[\"labels\"],test_size = 0.35)\n",
    "\n",
    "train_dataset_immuno_all,test_dataset_immuno = get_train_test_dataseqs(train_sequences.to_list(),test_sequences.to_list(),\n",
    "                                                                   train_labels,test_labels,tokenizer_iedb)\n",
    "\n",
    "ds_splits = train_dataset_immuno_all.train_test_split(test_size=0.1,seed =1)\n",
    "# train_dataset_immuno, eval_dataset_immuno = train_dataset_immuno.train_test_split(test_size=0.1)\n",
    "eval_dataset_immuno = ds_splits[\"test\"]\n",
    "train_dataset_immuno = ds_splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXCzMgMQ_Fp2"
   },
   "outputs": [],
   "source": [
    "eval_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6G-C4cI_Fp2"
   },
   "outputs": [],
   "source": [
    "train_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89mGW5S6_Fp2"
   },
   "outputs": [],
   "source": [
    "train_dataset_immuno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1IzBVbt_Fp2"
   },
   "outputs": [],
   "source": [
    "training_args_immuno = TrainingArguments(\n",
    "    # f\"{model_name}-finetuned-ft-humVir\",\n",
    "     f\"{model_name}-iedbImmuno_ft\",\n",
    "     overwrite_output_dir = True,\n",
    "    # num_train_epochs=1,\n",
    "    # f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "    per_device_train_batch_size=16,\n",
    "    # per_device_eval_batch_size=int(1.5*bch_size),\n",
    "#     gradient_accumulation_steps= 2, #4,\n",
    "    gradient_checkpointing=True,\n",
    "    # fp16=True,\n",
    "    bf16=True, # needs ampere, not supported ?\n",
    "    tf32=True,\n",
    "        torch_compile = True,\n",
    "    # optim = \"adamw_8bit\", #\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "    # optim= \"adamw_bnb_8bit\", #\"paged_adamw_8bit\",\n",
    "    label_names = [\"labels\"],\n",
    "    learning_rate = 3e-4,#2e-4 #5e-3,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # max_grad_norm = 0.95,\n",
    "    # eval_accumulation_steps = 2#8\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", #\"epoch\"#\"no\",\n",
    "        load_best_model_at_end=True,\n",
    "    # weight_decay = 0.001,\n",
    "    # output_dir=\".\",\n",
    "     no_cuda=False,\n",
    "     greater_is_better=True,\n",
    "     # save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "    # auto_find_batch_size = True, # new , reduces if oom\n",
    "    # num_train_epochs=num_epochs,\n",
    "\n",
    "    metric_for_best_model= \"eval_roc_auc\",#\"accuracy\",\n",
    "    # group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer_immuno_iedb = Trainer(model=merged_model, args=training_args_immuno,\n",
    "                  train_dataset=train_dataset_immuno,eval_dataset=eval_dataset_immuno,tokenizer=tokenizer_iedb,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "result_immuno_iedb = trainer_immuno_iedb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI4Ar1Qn_Fp3"
   },
   "outputs": [],
   "source": [
    "trainer_immuno_iedb.evaluate(test_dataset_immuno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsRgUWcQ_Fp3"
   },
   "outputs": [],
   "source": [
    "training_args_immuno.output_dir=\"iedb_base\"\n",
    "trainer_immuno_iedb_base = Trainer(model=base_model, args=training_args_immuno,\n",
    "                  train_dataset=train_dataset_immuno,eval_dataset=eval_dataset_immuno,tokenizer=tokenizer_iedb,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "result_immuno_iedb_base = trainer_immuno_iedb_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQYi0bOQ_Fp3"
   },
   "outputs": [],
   "source": [
    "trainer_immuno_iedb_base.evaluate(test_dataset_immuno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-W5Nriz_Fp3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1f6tWCmDttwj5GsdTavt7m-dIH1-2f1P5",
     "timestamp": 1706088386277
    },
    {
     "file_id": "18pa4xD0P5vwL-MX30nrkeye78W7403TZ",
     "timestamp": 1702551198722
    },
    {
     "file_id": "1uwM47KOjpzSn2TylouVCrTiGuskjQtEd",
     "timestamp": 1690279074762
    },
    {
     "file_id": "1_4meskHUbh7-dkjF-ajOT_wazp1vxzC6",
     "timestamp": 1630488037990
    },
    {
     "file_id": "1325niNI11d1AGvkudzFonUaYu-_IpRSv",
     "timestamp": 1629371463407
    },
    {
     "file_id": "1eeHOXM2csXDuY2ysmo-npc7dWtr9hcKJ",
     "timestamp": 1629281016353
    },
    {
     "file_id": "https://github.com/sacdallago/bio_embeddings/blob/develop/notebooks/embed_fasta_sequences.ipynb",
     "timestamp": 1629109653928
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4380199,
     "sourceId": 7529659,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
