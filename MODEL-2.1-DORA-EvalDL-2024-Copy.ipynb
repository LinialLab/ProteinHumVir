{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5w39f_fnY9"
   },
   "source": [
    "esm based dynamic model (not using static embeds).\n",
    "\n",
    "+ Use HF Trainer, LORA:\n",
    "  * https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "\n",
    "Use TF:\n",
    "*  🇰https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb#scrollTo=de8419b5\n",
    "* Torch based /Trainer example:  https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=49dcba23\n",
    "\n",
    "\n",
    "\n",
    "* NOte for pytorch training could use trainer maybe, and mixed precision - https://huggingface.co/docs/transformers/v4.18.0/en/performance#fp16-training\n",
    "\n",
    "\n",
    "* QLORA finetuning: https://huggingface.co/blog/AmelieSchreiber/esm2-ptm\n",
    "  * https://huggingface.co/blog/AmelieSchreiber/esmbind   (token level)\n",
    "\n",
    "* Another lora, qlora example - may use too much mem/bug : https://github.com/huggingface/peft/issues/1023\n",
    "* Default trainer (`AutoModelForSequenceClassification`) + Lora https://huggingface.co/docs/peft/task_guides/image_classification_lora\n",
    "   * seq cls with lora - maybe `task_type=\"SEQ_CLS\"` ? https://github.com/huggingface/peft/blob/main/docs/source/task_guides/ptuning-seq-classification.md\n",
    "* https://www.kaggle.com/code/andregrbnr/protein-sequence-classification - lora modules to save ??\n",
    "\n",
    "  * ESM2-Lora mem bug (also accel data loop) ? https://github.com/huggingface/peft/issues/1023\n",
    "\n",
    "\n",
    "* QLORA: https://huggingface.co/blog/AmelieSchreiber/esm2-ptm\n",
    "  * `36 batch size` with esm-150M !\n",
    "\n",
    "* lora peft - classifier layer weight saving issue?  https://github.com/huggingface/peft/issues/577\n",
    "\n",
    "note : we see overfit, worse perf when using more than 3 training epochs (lr 5e-4, DORA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3049,
     "status": "ok",
     "timestamp": 1706644628798,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "kalvvWZZgM0E",
    "outputId": "f323c22c-784c-4935-ef81-4b61183e3a3d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GO4tbwSU1pa9"
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge google-colab -y\n",
    "\n",
    "# !pip install tensorflow  -U -q # ankh\n",
    "\n",
    "# !pip install torch  -U -q # fair-esm # seqeval\n",
    "# !pip install transformers peft accelerate datasets evaluate bitsandbytes -U -q # --user\n",
    "# !pip install peft bitsandbytes -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_cRmHN0j709"
   },
   "source": [
    "* Use the unirpot fasta file I downloaded and uploaded to my drive\n",
    "\n",
    "`/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_90.fasta`\n",
    "\n",
    "* Download fasta from: `https://www.uniprot.org/uniref/?query=uniprot:(keyword%3A%22Transmembrane+%5BKW-0812%5D%22+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22)+identity:0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kwgVw-CN1OgJ"
   },
   "outputs": [],
   "source": [
    "#### DATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/swp_human_viri_all_embed_esm.parquet\" ## ESM1B embedding (max len 1022)\n",
    "# DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-train.csv.gz\"## TRAIN\n",
    "# DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "# DATA_PATH = \"/kaggle/input/humvir-proteins/hum_vir_swp-globalEmbed-train.csv/hum_vir_swp-globalEmbed-train.csv\"\n",
    "DATA_PATH = \"hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "\n",
    "## TEST data:\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-test.csv.gz\"## TRAIN\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "TEST_DATA_PATH = \"hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "\n",
    "# ## metadata for all reviewed/swissprot human + virus proteins\n",
    "# METADATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/SWP_human_viruses_all.xlsx\"\n",
    "\n",
    "TARGET_COL = \"virus\" ## use for filtering data into 1 class\n",
    "\n",
    "MAX_LEN = 1024#768#530#480 # exclude sequences longer than this. (Not merely truncate)\n",
    "\n",
    "FAST_RUN = False#True\n",
    "SAVE_MODEL = False#True\n",
    "SAVE_OUTPUTS = True\n",
    "\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/trained_esm_lora_trainer_model\"\n",
    "MODEL_DRIVE_SAVE_PATH = \"dora_trained_esm_lora_trainer_model\"\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/kaggle/input/humvir-proteins/qlora/qlora\" # saved + reuploadedon kaggle\n",
    "\n",
    "TRAIN_MODEL = True#False\n",
    "LOAD_TRAINED = False#True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa-7WUuSfnZC"
   },
   "source": [
    "# Embed sequences in a FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11612,
     "status": "ok",
     "timestamp": 1706644666124,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "hKw7JD3_fnZD",
    "outputId": "ffa04bad-8a62-447c-f1cd-466ebec4c602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_945/2867410804.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2024-03-02 13:54:29.121926: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-02 13:54:29.138120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-02 13:54:29.138138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-02 13:54:29.138664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-02 13:54:29.142137: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-02 13:54:29.551449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "# from Bio import SeqIO\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "# from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "## https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "# from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "# import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForSequenceClassification, AutoPeftModel\n",
    "\n",
    "## could use transformer pipeline for inference;\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "# from tqdm.auto import tqdm\n",
    "# pipe = pipeline(\"text-classification\", model=\"facebook/wav2vec2-base-960h\", device=0)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"humVir_evalDLTest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(mode=\"disabled\")\n",
    "# # alt \n",
    "# wandb.init(project='qlora_humvir')# ; or args = TrainingArguments(report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r8gFb91iUc8N"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "# # Use the accelerator\n",
    "# ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # שבבקךקרשאםר צשטנק בשודקד ןדדוקד?\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\") #\"bf16\") #bf16\") # fp16 # orig used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H1bosQnEwXvf"
   },
   "outputs": [],
   "source": [
    "num_epochs = 4#4\n",
    "bch_size = 32#36#32#8#3#2\n",
    "\n",
    "# opt = Adafactor(3e-4)##AdamWeightDecay(1e-4) #default: AdamWeightDecay(2e-5)\n",
    "# opt = AdamWeightDecay(5e-4)#(1e-3)\n",
    "# opt = Adam(8e-4)\n",
    "\n",
    "if FAST_RUN:\n",
    "    num_epochs = 2\n",
    "    # bch_size = 16\n",
    "    bch_size = 32\n",
    "    MAX_LEN = int(MAX_LEN//1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dEGNJ6xlxQvC"
   },
   "outputs": [],
   "source": [
    "# model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t30_150M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t33_650M_UR50D\"\n",
    "models_list = [\"facebook/esm2_t6_8M_UR50D\",\"facebook/esm2_t12_35M_UR50D\",\"facebook/esm2_t30_150M_UR50D\",\"facebook/esm2_t33_650M_UR50D\"]\n",
    "if FAST_RUN:\n",
    "#     model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "  # model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n",
    "    models_list = [\"facebook/esm2_t6_8M_UR50D\",\"facebook/esm2_t12_35M_UR50D\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4653,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "Eggiq94CubAw",
    "outputId": "9ab29551-cfb8-47d0-9b6d-ff6457e3ca09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20340\n",
      "Sequence        20340\n",
      "virus               2\n",
      "Length           1454\n",
      "Cluster name    16810\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>virus</th>\n",
       "      <th>Length</th>\n",
       "      <th>Cluster name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>Cluster: DET1- and DDB1-associated protein 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...</td>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>Cluster: Protein MGF 360-19R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>Cluster: Pyrin domain-containing protein 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>Cluster: Protein FAM9B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...</td>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>Cluster: TRAF2 and NCK-interacting protein kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20335</th>\n",
       "      <td>MDPDKQDALNSIENSIYRTAFKLQSVQTLCQLDLIDSSLIQQVLLR...</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>Cluster: Dystrotelin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20336</th>\n",
       "      <td>MLCPWRTANLGLLLILTIFLVAEAEGAAQPNNSLMLQTSKENHALA...</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>Cluster: Cell surface glycoprotein CD200 recep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>MCLRFFSPVPGSTSSATNVTMVVSAGPWSSEKAEMNILEINEKLRP...</td>\n",
       "      <td>0</td>\n",
       "      <td>421</td>\n",
       "      <td>Cluster: Putative neuroblastoma breakpoint fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20338</th>\n",
       "      <td>MASHAGQQHAPAFGQAARASGPTDGRAASRPSHRQGASEARGDPEL...</td>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>Cluster: Thymidine kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20339</th>\n",
       "      <td>MSASSLLEQRPKGQGNKVQNGSVHQKDGLNDDDFEPYLSPQARPNN...</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>Cluster: YTH domain-containing family protein 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20340 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sequence  virus  Length  \\\n",
       "0      MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...      0     102   \n",
       "1      MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...      1     362   \n",
       "2      MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...      0      97   \n",
       "3      MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...      0     186   \n",
       "4      MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...      0    1360   \n",
       "...                                                  ...    ...     ...   \n",
       "20335  MDPDKQDALNSIENSIYRTAFKLQSVQTLCQLDLIDSSLIQQVLLR...      0     578   \n",
       "20336  MLCPWRTANLGLLLILTIFLVAEAEGAAQPNNSLMLQTSKENHALA...      0     348   \n",
       "20337  MCLRFFSPVPGSTSSATNVTMVVSAGPWSSEKAEMNILEINEKLRP...      0     421   \n",
       "20338  MASHAGQQHAPAFGQAARASGPTDGRAASRPSHRQGASEARGDPEL...      1     376   \n",
       "20339  MSASSLLEQRPKGQGNKVQNGSVHQKDGLNDDDFEPYLSPQARPNN...      0     579   \n",
       "\n",
       "                                            Cluster name  \n",
       "0           Cluster: DET1- and DDB1-associated protein 1  \n",
       "1                           Cluster: Protein MGF 360-19R  \n",
       "2             Cluster: Pyrin domain-containing protein 2  \n",
       "3                                 Cluster: Protein FAM9B  \n",
       "4      Cluster: TRAF2 and NCK-interacting protein kinase  \n",
       "...                                                  ...  \n",
       "20335                               Cluster: Dystrotelin  \n",
       "20336  Cluster: Cell surface glycoprotein CD200 recep...  \n",
       "20337  Cluster: Putative neuroblastoma breakpoint fam...  \n",
       "20338                          Cluster: Thymidine kinase  \n",
       "20339    Cluster: YTH domain-containing family protein 2  \n",
       "\n",
       "[20340 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_parquet(DATA_PATH) # numpy to pandas\n",
    "df = pd.read_csv(DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    "df_test = pd.read_csv(TEST_DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    " ## lengths of all the seqs\n",
    "    \n",
    "## concat all data - for CV\n",
    "# df = pd.concat([df,df_test])\n",
    "print(df.shape[0])\n",
    "print(df.nunique())\n",
    "\n",
    "if FAST_RUN:\n",
    "#   # df.loc[df[\"Length\"]>200]\n",
    "    df = df.sample(frac=0.01)\n",
    "    df_test = df_test.sample(frac=0.02)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[df[\"Length\"]>500].sample(500)\n",
    "# print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.groupby([\"virus\"])[\"Cluster name\"].nunique().describe().round(2)\n",
    "except:()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "fkeYBgfafVoy",
    "outputId": "e8f72139-fcad-41d6-f9df-7fb73571df63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14668.0</td>\n",
       "      <td>473.16</td>\n",
       "      <td>308.52</td>\n",
       "      <td>11.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>1533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5672.0</td>\n",
       "      <td>380.31</td>\n",
       "      <td>298.35</td>\n",
       "      <td>11.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1520.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count    mean     std   min    25%    50%    75%     max\n",
       "virus                                                            \n",
       "0      14668.0  473.16  308.52  11.0  244.0  398.0  623.0  1533.0\n",
       "1       5672.0  380.31  298.35  11.0  151.0  297.0  514.0  1520.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"virus\"])[\"Length\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "rm2Ms4BmQ4wn",
    "outputId": "a8edf73e-0541-43dd-8537-10b64de09661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean         0.28\n",
      "sum       5672.00\n",
      "count    20340.00\n",
      "Name: virus, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    20340.0\n",
       "mean       447.0\n",
       "std        309.0\n",
       "min         11.0\n",
       "25%        213.0\n",
       "50%        372.0\n",
       "75%        592.0\n",
       "max       1533.0\n",
       "Name: Length, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"virus\"].agg([\"mean\",\"sum\",\"count\"]).round(2))\n",
    "df[\"Length\"].describe().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "F8VNnHUwRQRm",
    "outputId": "c5502a31-ec5d-417d-fb9e-87e1de7d358d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    20340.0\n",
      "mean       447.3\n",
      "std        308.5\n",
      "min         11.0\n",
      "25%        213.0\n",
      "50%        372.0\n",
      "75%        592.2\n",
      "max       1533.0\n",
      "Name: Length, dtype: float64\n",
      "mean         0.28\n",
      "sum       5672.00\n",
      "count    20340.00\n",
      "Name: virus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# df = df.loc[df[\"Length\"]<=2*MAX_LEN].reset_index(drop=True)\n",
    "# df_test = df_test.loc[df_test[\"Length\"]<=2*MAX_LEN].reset_index(drop=True)\n",
    "\n",
    "print(df[\"Length\"].describe().round(1))\n",
    "print(df[\"virus\"].agg([\"mean\",\"sum\",\"count\"]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence        20340\n",
       "virus               2\n",
       "Length           1454\n",
       "Cluster name    16810\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PcMwJBkUov07"
   },
   "outputs": [],
   "source": [
    "# ## metadata about all sequences, can be used to identify and to define targets/labels\n",
    "# df_meta = pd.read_excel(METADATA_PATH).dropna(how=\"all\",axis=1)\n",
    "# df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSXy_OzjhIB7"
   },
   "source": [
    "### example pretrained fb/torch:\n",
    "* https://github.com/facebookresearch/esm#getting-started-with-this-repo-\n",
    "\n",
    "* Transformers + trainer example : (mlm case): https://github.com/facebookresearch/esm/discussions/556\n",
    "* keras models supported / via HF?\n",
    "  * https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "\n",
    "\n",
    "  TF finetuning example (sequence evel?):\n",
    "  https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb#scrollTo=4b26b828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CDdMQ98rii9t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForTokenClassification, TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tcssXXfaVYns"
   },
   "outputs": [],
   "source": [
    "ID2LABEL = {\n",
    "    0: \"Human\",\n",
    "    1: \"Virus\"\n",
    "}\n",
    "LABEL2ID = {v: k for k, v in ID2LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rR9-cKgPjzW2"
   },
   "outputs": [],
   "source": [
    "train_sequences = df[\"Sequence\"].tolist()\n",
    "train_labels = df[\"virus\"].tolist()\n",
    "train_groups = df[\"Cluster name\"].tolist()\n",
    "\n",
    "# # train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)#,stratify=labels)\n",
    "\n",
    "test_sequences = df_test[\"Sequence\"].tolist()\n",
    "test_labels = df_test[\"virus\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(base_model_name_or_path=model_checkpoint,\n",
    "# #                         init_lora_weights=\"loftq\", loftq_config=loftq_config,\n",
    "#     task_type= \"SEQ_CLS\",#TaskType.SEQ_CLS, ## disabling helps?? (then get \"ValueError: Attempting to unscale FP16 gradients.\")\n",
    "#     inference_mode=False, r= 2 if FAST_RUN else 4, #16,\n",
    "#     lora_alpha=8,\n",
    "# #     lora_dropout=0.05,\n",
    "#     use_rslora = True,\n",
    "#     bias= \"all\"#\"lora_only\",#\"none\",#\"lora_only\",#\"none\",#\"all\",\n",
    "#     # target_modules=[\n",
    "#     #     \"query\", \"key\", \"value\",\n",
    "#     #                 \"EsmSelfOutput.dense\",\n",
    "#     #         \"EsmIntermediate.dense\",\n",
    "#     #         \"EsmOutput.dense\",\n",
    "#     #                 # \"word_embeddings\",\n",
    "#     #                 # \"EsmClassificationHead.dense\", ## not sure if works/changes anything\n",
    "#     #                 # \"out_proj\",\n",
    "#     #                 # \"classifier\" # fails\n",
    "#     #                 ],\n",
    "# #         target_modules=  target_modules#\"all-linear\"#modules_list,\n",
    "#            ,target_modules=  \"all-linear\"\n",
    "                         \n",
    "#     # # ### https://www.kaggle.com/code/andregrbnr/protein-sequence-classification\n",
    "# #      ,modules_to_save= \"all-linear\",\n",
    "# #                          [#\"decode_head\",\n",
    "# # #                       \"classifier\",\n",
    "# #          \"EsmClassificationHead\",\n",
    "# #          'classifier.dense', 'classifier.out_proj',\n",
    "# #          \"pooler\",\n",
    "# # # #                      'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "# #                      ]\n",
    "#     ## 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "#     # modules_to_save=[\"classifier\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mEoRinaIznOa"
   },
   "outputs": [],
   "source": [
    "##https://huggingface.co/docs/peft/main/en/developer_guides/quantization\n",
    "## lotfQ config - for this, do not initialize as quantized!\n",
    "# from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "# loftq_config = LoftQConfig(loftq_bits=4)\n",
    "\n",
    "## https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True, # disable to train ok;\n",
    "#   load_in_8bit=True, # alt\n",
    "# load_in_4bit=False,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "  # llm_int8_has_fp16_weight= True, # try alt\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1706644674751,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "k_KUug1Lh5bH",
    "outputId": "14e7d50d-cdfe-4965-9f83-704a4a821e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load ESM-2 model\n",
    "## smallest: esm2_t6_8M_UR50D\n",
    "## 2d smallest\n",
    "## large: esm2_t33_650M_UR50D\n",
    "\n",
    " # ElnaggarLab/ankh-base\n",
    " ### https://github.com/agemagician/Ankh/blob/main/examples/binary_classification_solubility_task.ipynb - different model?\n",
    " #  https://github.com/agemagician/Ankh#models   - 450M model size # model_checkpoint =   \"ElnaggarLab/ankh-base\"\n",
    "\n",
    "model_max_len = min(1024,MAX_LEN) # 800\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "# #                                           padding= False#True # orig\n",
    "#                                           padding= True# alt\n",
    "#                                           ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "num_labels = 2 # max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "print(\"Num labels:\", num_labels)\n",
    "##ORIG:\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\") # worked, orig\n",
    "\n",
    "## try this now, alt:\n",
    "# model = EsmForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "# model = TFEsmForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\")\n",
    "\n",
    "## https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=fc164b49 # uses trainer\n",
    "# ##\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, #num_labels=num_labels,\n",
    "# #                                                            problem_type=\"single_label_classification\", # was enabled\n",
    "# #                                                            load_in_4bit=True, # disable to train ok\n",
    "# #                                                            load_in_4bit= False,\n",
    "# #                                                             quantization_config=nf4_config,\n",
    "#                                                           #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "# #                                                            device_map= \"cuda:0\",#\"auto\",\n",
    "#                                                            device_map=\"auto\",\n",
    "#                                                           num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "# #                                                             trust_remote_code=True,\n",
    "#                                                           )\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # prepares the whole model for kbit training\n",
    "\n",
    "# last_layer_num = model.num_layers ## 33 for esm2_t33_650M_UR50D\n",
    "# print(last_layer_num )\n",
    "\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_model_lora(model_checkpoint):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                           problem_type=\"single_label_classification\", # was enabled\n",
    "#                                                            load_in_4bit=True, \n",
    "#                                                             quantization_config=nf4_config,\n",
    "                                                          #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "                                                           device_map=\"auto\",\n",
    "                                                          num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "                                                          )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # prepares the whole model for kbit training\n",
    "    peft_config = LoraConfig(base_model_name_or_path=model_checkpoint,\n",
    "    task_type= \"SEQ_CLS\",inference_mode=False, r= 2 if FAST_RUN else 8, #16,\n",
    "    use_dora=True,                         \n",
    "    lora_alpha=8,use_rslora = True,\n",
    "                             bias= \"all\",target_modules=  \"all-linear\"\n",
    "    # # ### https://www.kaggle.com/code/andregrbnr/protein-sequence-classification\n",
    "#      ,modules_to_save= \"all-linear\",\n",
    "#                          [#\"decode_head\",\n",
    "# #                       \"classifier\",\n",
    "#          \"EsmClassificationHead\",\n",
    "#          'classifier.dense', 'classifier.out_proj',\n",
    "#          \"pooler\",\n",
    "# # #                      'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "#                      ]\n",
    ")\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_raw_model_lora(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After we wrap our base model model with PeftModel along with the config, we get a new model where only the LoRA parameters are trainable (so-called “update matrices”) while the pre-trained parameters are kept frozen. These include the parameters of the randomly initialized classifier parameters too. This is NOT we want when fine-tuning the base model on our custom dataset. To ensure that the classifier parameters are also trained, we specify modules_to_save. This also ensures that these modules are serialized alongside the LoRA trainable parameters when using utilities like save_pretrained() and push_to_hub().\n",
    "\n",
    "In addition to specifying the target_modules within LoraConfig, we also need to specify the modules_to_save. When we wrap our base model with PeftModel and pass the configuration, we obtain a new model in which only the LoRA parameters are trainable, while the pre-trained parameters and the randomly initialized classifier parameters are kept frozen. However, we do want to train the classifier parameters. By specifying the modules_to_save argument, we ensure that the classifier parameters are also trainable, and they will be serialized alongside the LoRA trainable parameters when we use utility functions like save_pretrained() and push_to_hub().\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Aw4wKUDNFnBj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20340\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Warning - longer than allowed length - 1024\n",
    "# \"\"\"\n",
    "\n",
    "# same tokenizer for all?\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\",\n",
    "#                                           padding= False#True # orig\n",
    "                                          padding= True# alt\n",
    "                                          ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "train_tokenized = tokenizer(train_sequences,  truncation=True,max_length=model_max_len,padding=True) # padding=True,\n",
    "test_tokenized = tokenizer(test_sequences,  truncation=True,max_length=model_max_len,padding=True) # padding=True,\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "y_4o9iEZFLBZ"
   },
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# # # Use the accelerator\n",
    "# # ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # # # accelerator = Accelerator(mixed_precision=\"fp16\") # fp16 # orig used\n",
    "# model = accelerator.prepare(model)\n",
    "\n",
    "train_dataset = accelerator.prepare(train_dataset)\n",
    "test_dataset = accelerator.prepare(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEcix1ZWfQg-"
   },
   "source": [
    "With 35M model :\n",
    "```\n",
    "Default (1024) max length\n",
    "batch_size=8\n",
    "\n",
    "opt = Adafactor(1e-4)##AdamWeightDecay(1e-4) #default: AdamWeightDecay(2e-5)\n",
    "model.compile(optimizer=opt, metrics=[\"accuracy\"],\n",
    "              loss=\"BinaryCrossentropy\")\n",
    "3813/3813 [==============================] - 2625s 661ms/step - loss: 0.2452 - accuracy: 0.9104 - val_loss: 0.1622 - val_accuracy: 0.9363\n",
    "Epoch 2/3\n",
    "3813/3813 [==============================] - 2518s 661ms/step - loss: 0.1218 - accuracy: 0.9597 - val_loss: 0.1533 - val_accuracy: 0.9463\n",
    "Epoch 3/3\n",
    "3813/3813 [==============================] - 2523s 662ms/step - loss: 0.0730 - accuracy: 0.9799 - val_loss: 0.1978 - val_accuracy: 0.9436\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Q_3tSKYxJJtE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,roc_auc_score\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_metric\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "    \n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"roc_auc\":roc_auc_score(labels,pred.predictions[:,1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hbj58ofX8umQ",
    "outputId": "a60712d1-b63b-4c6f-a988-d8e8b50a0c77"
   },
   "outputs": [],
   "source": [
    "# if TRAIN_MODEL:\n",
    "#     result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facebook/esm2_t6_8M_UR50D',\n",
       " 'facebook/esm2_t12_35M_UR50D',\n",
       " 'facebook/esm2_t30_150M_UR50D',\n",
       " 'facebook/esm2_t33_650M_UR50D']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 427,550 || all params: 8,254,890 || trainable%: 5.179354297876774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mddofer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e/Stuff/Research/datasets/New Protein-Virus anom project/wandb/run-20240302_135455-lsz74bs3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ddofer/humVir_evalDLTest/runs/lsz74bs3' target=\"_blank\">esm2_t6_8M_UR50D-DORA-finetuned-ft-humVir</a></strong> to <a href='https://wandb.ai/ddofer/humVir_evalDLTest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ddofer/humVir_evalDLTest' target=\"_blank\">https://wandb.ai/ddofer/humVir_evalDLTest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ddofer/humVir_evalDLTest/runs/lsz74bs3' target=\"_blank\">https://wandb.ai/ddofer/humVir_evalDLTest/runs/lsz74bs3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2544' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2544/2544 43:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.919615</td>\n",
       "      <td>0.919615</td>\n",
       "      <td>0.919615</td>\n",
       "      <td>0.919615</td>\n",
       "      <td>0.974837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.176723</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.980692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.154016</td>\n",
       "      <td>0.949131</td>\n",
       "      <td>0.949131</td>\n",
       "      <td>0.949131</td>\n",
       "      <td>0.949131</td>\n",
       "      <td>0.982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.199613</td>\n",
       "      <td>0.951434</td>\n",
       "      <td>0.951434</td>\n",
       "      <td>0.951434</td>\n",
       "      <td>0.951434</td>\n",
       "      <td>0.981257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t12_35M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,190,910 || all params: 35,132,930 || trainable%: 3.3897258213305865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2544' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2544/2544 1:46:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.203900</td>\n",
       "      <td>0.188671</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.930919</td>\n",
       "      <td>0.984140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.133338</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.987548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.160870</td>\n",
       "      <td>0.951643</td>\n",
       "      <td>0.951643</td>\n",
       "      <td>0.951643</td>\n",
       "      <td>0.951643</td>\n",
       "      <td>0.987867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.985753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-636 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-1272 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-1908 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-2544 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t30_150M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,581,790 || all params: 152,182,730 || trainable%: 2.3536113460443246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2544' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2544/2544 10:59:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.105435</td>\n",
       "      <td>0.959389</td>\n",
       "      <td>0.959389</td>\n",
       "      <td>0.959389</td>\n",
       "      <td>0.959389</td>\n",
       "      <td>0.990203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.134699</td>\n",
       "      <td>0.957505</td>\n",
       "      <td>0.957505</td>\n",
       "      <td>0.957505</td>\n",
       "      <td>0.957505</td>\n",
       "      <td>0.991375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.129842</td>\n",
       "      <td>0.968181</td>\n",
       "      <td>0.968181</td>\n",
       "      <td>0.968181</td>\n",
       "      <td>0.968181</td>\n",
       "      <td>0.991806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.143599</td>\n",
       "      <td>0.969228</td>\n",
       "      <td>0.969228</td>\n",
       "      <td>0.969228</td>\n",
       "      <td>0.969228</td>\n",
       "      <td>0.991651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-636 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-1272 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-1908 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-2544 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 04:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t33_650M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,608,190 || all params: 660,530,790 || trainable%: 1.3032231245420065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='491' max='2544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 491/2544 13:56:36 < 58:32:21, 0.01 it/s, Epoch 0.77/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_eval_dict = {}\n",
    "\n",
    "## could log models vs run properly, skip for now \n",
    "## https://docs.wandb.ai/guides/track/log/log-models\n",
    "\n",
    "for model_checkpoint in models_list:\n",
    "    # ### AttributeError: 'TFEsmForSequenceClassification' object has no attribute 'to'\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    print(model_name)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        run_name=f\"{model_name}-DORA-finetuned-FastRun{FAST_RUN}-humVir\",\n",
    "    #     f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "        per_device_train_batch_size=bch_size,\n",
    "        overwrite_output_dir = True,\n",
    "        # per_device_eval_batch_size=int(2*bch_size),\n",
    "        # gradient_accumulation_steps= 2, #4,\n",
    "        gradient_checkpointing=True, # maybe causes slowdown? \n",
    "        # fp16=True,\n",
    "        bf16=True, # needs ampere, not supported\n",
    "        tf32=True,\n",
    "    #         torch_compile = True,\n",
    "        optim = \"adamw_8bit\",#\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "    #     optim=\"paged_adamw_32bit\",\n",
    "        label_names = [\"labels\"],\n",
    "        learning_rate = 5e-4,#5e-4, #5e-3,\n",
    "        # lr_scheduler_type=\"cosine\",\n",
    "        max_grad_norm = 0.95,\n",
    "    #     weight_decay=0.001,\n",
    "        # eval_accumulation_steps = 2#8\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # warmup_ratio=0.02,\n",
    "        save_strategy= \"epoch\",#\"no\",\n",
    "        output_dir= \".\", # f\"{model_name}-DORA-finetuned-FastRun{FAST_RUN}-humVir\", ?\n",
    "         no_cuda=False,\n",
    "         greater_is_better=True,\n",
    "         # save_total_limit=1,\n",
    "      remove_unused_columns=False,\n",
    "        auto_find_batch_size = True, # new , reduces if oom\n",
    "        num_train_epochs=num_epochs,\n",
    "        # load_best_model_at_end=True,\n",
    "        metric_for_best_model= \"eval_roc_auc\",#\"accuracy\",\n",
    "        group_by_length=True,\n",
    "    \n",
    "        report_to=\"wandb\",  # enable logging to W&B\n",
    "        # run_name=\"bert-base-high-lr\",  # name of the W&B run (optional)\n",
    "        # logging_steps=1,  # how often to log to W&B\n",
    "    )\n",
    "\n",
    "    model = get_raw_model_lora(model_checkpoint)\n",
    "    trainer = Trainer(model=model, args=training_args,\n",
    "                  train_dataset=train_dataset,eval_dataset=test_dataset,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "    train_result = trainer.train()\n",
    "    result_eval_dict[model_name] =  train_result.metrics\n",
    "    result_eval_dict[model_name].update(trainer.evaluate())\n",
    "    # result_eval_dict[model_name] = trainer.evaluate()\n",
    "print(result_eval_dict[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval_dict[model_name] =  train_result.metrics\n",
    "result_eval_dict[model_name].update(trainer.evaluate())\n",
    "# result_eval_dict[model_name] = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval = pd.DataFrame(result_eval_dict).T\n",
    "display(result_eval)\n",
    "if SAVE_OUTPUTS:\n",
    "    result_eval.to_csv(f\"DORA_DL_eval_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>esm2_t6_8M_UR50D</th>\n",
       "      <td>1728.3349</td>\n",
       "      <td>35.306</td>\n",
       "      <td>2.208</td>\n",
       "      <td>2.906153e+15</td>\n",
       "      <td>0.180384</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.186385</td>\n",
       "      <td>0.940548</td>\n",
       "      <td>0.940548</td>\n",
       "      <td>0.940548</td>\n",
       "      <td>0.940548</td>\n",
       "      <td>0.977460</td>\n",
       "      <td>33.4969</td>\n",
       "      <td>142.610</td>\n",
       "      <td>17.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esm2_t12_35M_UR50D</th>\n",
       "      <td>3938.9578</td>\n",
       "      <td>15.491</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.280104e+16</td>\n",
       "      <td>0.132981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.135253</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.951015</td>\n",
       "      <td>0.986715</td>\n",
       "      <td>75.9346</td>\n",
       "      <td>62.909</td>\n",
       "      <td>7.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esm2_t30_150M_UR50D</th>\n",
       "      <td>11260.8862</td>\n",
       "      <td>5.419</td>\n",
       "      <td>0.339</td>\n",
       "      <td>5.621005e+16</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.083135</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.996908</td>\n",
       "      <td>214.2689</td>\n",
       "      <td>22.294</td>\n",
       "      <td>2.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esm2_t33_650M_UR50D</th>\n",
       "      <td>11260.8862</td>\n",
       "      <td>5.419</td>\n",
       "      <td>0.339</td>\n",
       "      <td>5.621005e+16</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.083135</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.996908</td>\n",
       "      <td>214.2689</td>\n",
       "      <td>22.294</td>\n",
       "      <td>2.791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_runtime  train_samples_per_second  \\\n",
       "esm2_t6_8M_UR50D         1728.3349                    35.306   \n",
       "esm2_t12_35M_UR50D       3938.9578                    15.491   \n",
       "esm2_t30_150M_UR50D     11260.8862                     5.419   \n",
       "esm2_t33_650M_UR50D     11260.8862                     5.419   \n",
       "\n",
       "                     train_steps_per_second    total_flos  train_loss  epoch  \\\n",
       "esm2_t6_8M_UR50D                      2.208  2.906153e+15    0.180384    3.0   \n",
       "esm2_t12_35M_UR50D                    0.969  1.280104e+16    0.132981    3.0   \n",
       "esm2_t30_150M_UR50D                   0.339  5.621005e+16    0.084878    3.0   \n",
       "esm2_t33_650M_UR50D                   0.339  5.621005e+16    0.084878    3.0   \n",
       "\n",
       "                     eval_loss  eval_accuracy   eval_f1  eval_precision  \\\n",
       "esm2_t6_8M_UR50D      0.186385       0.940548  0.940548        0.940548   \n",
       "esm2_t12_35M_UR50D    0.135253       0.951015  0.951015        0.951015   \n",
       "esm2_t30_150M_UR50D   0.083135       0.973833  0.973833        0.973833   \n",
       "esm2_t33_650M_UR50D   0.083135       0.973833  0.973833        0.973833   \n",
       "\n",
       "                     eval_recall  eval_roc_auc  eval_runtime  \\\n",
       "esm2_t6_8M_UR50D        0.940548      0.977460       33.4969   \n",
       "esm2_t12_35M_UR50D      0.951015      0.986715       75.9346   \n",
       "esm2_t30_150M_UR50D     0.973833      0.996908      214.2689   \n",
       "esm2_t33_650M_UR50D     0.973833      0.996908      214.2689   \n",
       "\n",
       "                     eval_samples_per_second  eval_steps_per_second  \n",
       "esm2_t6_8M_UR50D                     142.610                 17.852  \n",
       "esm2_t12_35M_UR50D                    62.909                  7.875  \n",
       "esm2_t30_150M_UR50D                   22.294                  2.791  \n",
       "esm2_t33_650M_UR50D                   22.294                  2.791  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result_eval = pd.DataFrame(result_eval_dict).T\n",
    "# display(result_eval)\n",
    "# if SAVE_OUTPUTS:\n",
    "#     result_eval.to_csv(f\"DL_eval_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▄▄▅▆▆▆▆▇▇▇███</td></tr><tr><td>eval/f1</td><td>▁▄▄▄▅▆▆▆▆▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▅▆▆▄▃▄▃▂▂▃▃▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▄▄▄▅▆▆▆▆▇▇▇███</td></tr><tr><td>eval/recall</td><td>▁▄▄▄▅▆▆▆▆▇▇▇███</td></tr><tr><td>eval/roc_auc</td><td>▁▃▃▃▄▆▅▆▆▇▇▇███</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▂▂▂▂▅▅▅▅███</td></tr><tr><td>eval/samples_per_second</td><td>████▄▄▄▄▂▂▂▂▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>████▄▄▄▄▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▅▆▇██▁▂▃▃▄▅▅▆▇██▁▂▃▃▄▅▅▆███▁▂▃▃▄▅▅</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▅▆▇██▁▂▃▃▄▅▅▆▇██▁▂▃▃▄▅▅▆███▁▂▃▃▄▅▅</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁█▇▆▅▃▂▁█▇▆▅▃▂▁█▇▆▄▃</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▄▃▇▅▄▃▃▂▂▅▃▃▂▂▁▁▄▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁▂█</td></tr><tr><td>train/train_loss</td><td>█▅▁</td></tr><tr><td>train/train_runtime</td><td>▁▃█</td></tr><tr><td>train/train_samples_per_second</td><td>█▃▁</td></tr><tr><td>train/train_steps_per_second</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97383</td></tr><tr><td>eval/f1</td><td>0.97383</td></tr><tr><td>eval/loss</td><td>0.08313</td></tr><tr><td>eval/precision</td><td>0.97383</td></tr><tr><td>eval/recall</td><td>0.97383</td></tr><tr><td>eval/roc_auc</td><td>0.99691</td></tr><tr><td>eval/runtime</td><td>387.8401</td></tr><tr><td>eval/samples_per_second</td><td>12.317</td></tr><tr><td>eval/steps_per_second</td><td>1.542</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>2544</td></tr><tr><td>train/learning_rate</td><td>7e-05</td></tr><tr><td>train/loss</td><td>0.045</td></tr><tr><td>train/total_flos</td><td>5.62100528277504e+16</td></tr><tr><td>train/train_loss</td><td>0.08488</td></tr><tr><td>train/train_runtime</td><td>11260.8862</td></tr><tr><td>train/train_samples_per_second</td><td>5.419</td></tr><tr><td>train/train_steps_per_second</td><td>0.339</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">esm2_t6_8M_UR50D-finetuned-ft-humVir</strong> at: <a href='https://wandb.ai/ddofer/humVir_evalDLTest/runs/qendwjyl' target=\"_blank\">https://wandb.ai/ddofer/humVir_evalDLTest/runs/qendwjyl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240218_000438-qendwjyl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run CV\n",
    "\n",
    "150M , full data, withgradient checkpoint enabled: 2.2 min per epoch (4.4 total), for 500 (long) samples.\n",
    "```\n",
    " [42/42 04:41, Epoch 2/2]\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\tF1\tPrecision\tRecall\tRoc Auc\n",
    "1\tNo log\t0.344198\t0.868263\t0.868263\t0.868263\t0.868263\t0.938507\n",
    "2\tNo log\t0.255124\t0.880240\t0.880240\t0.880240\t0.880240\t0.958227\n",
    "```\n",
    "similar time when not using gradient checkpoint and double (32) batch szie - 4 min for. epochs? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1f6tWCmDttwj5GsdTavt7m-dIH1-2f1P5",
     "timestamp": 1706088386277
    },
    {
     "file_id": "18pa4xD0P5vwL-MX30nrkeye78W7403TZ",
     "timestamp": 1702551198722
    },
    {
     "file_id": "1uwM47KOjpzSn2TylouVCrTiGuskjQtEd",
     "timestamp": 1690279074762
    },
    {
     "file_id": "1_4meskHUbh7-dkjF-ajOT_wazp1vxzC6",
     "timestamp": 1630488037990
    },
    {
     "file_id": "1325niNI11d1AGvkudzFonUaYu-_IpRSv",
     "timestamp": 1629371463407
    },
    {
     "file_id": "1eeHOXM2csXDuY2ysmo-npc7dWtr9hcKJ",
     "timestamp": 1629281016353
    },
    {
     "file_id": "https://github.com/sacdallago/bio_embeddings/blob/develop/notebooks/embed_fasta_sequences.ipynb",
     "timestamp": 1629109653928
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4380199,
     "sourceId": 7529659,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
